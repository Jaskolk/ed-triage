{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returning to this approach after some delay working on neural network approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Online, we import the usual packages. **xgboost** needs to be installed (with conda install xgboost or pip install -U xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now the usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn.metrics\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random as rn\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_analysis(preds, targets):\n",
    "    admission_thresholds = [0.01, 0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    i = 0\n",
    "    for thresh in admission_thresholds:\n",
    "        thresholded_predictions = [0 if prob[0] >= thresh else 1 for prob in preds]\n",
    "        \n",
    "        f1_w, f1, acc, prec, rec,= get_metrics(targets,thresholded_predictions, print_output = False)\n",
    "\n",
    "        if i == 0:\n",
    "            output_df = pd.DataFrame([thresh, f1_w, f1, acc, prec, rec, rec[0], rec[1]]).T\n",
    "            output_df.columns = ['admission_threshold', 'weighted f1', 'f1', 'accuracy', 'precision', 'recall', 'admission sensitivity', 'admission specificity']\n",
    "        else:\n",
    "            output_df.loc[len(output_df)] = [thresh, f1_w, f1, acc, prec, rec, rec[0], rec[1]]\n",
    "        i+=1\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "zoR-h8cO52Se",
    "outputId": "17db8de9-e516-4064-ec30-42bdd9542f65"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(targets, predictions, labels):\n",
    "    LABELS = labels\n",
    "\n",
    "    confusion_matrix = sklearn.metrics.confusion_matrix(targets, predictions)\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    sns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", annot_kws={\"size\": 15});\n",
    "    plt.title(\"Confusion matrix\", fontsize=10)\n",
    "    plt.ylabel('True label', fontsize=10)\n",
    "    plt.xlabel('Predicted label', fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test, preds, print_output = False):\n",
    "    f1_w = sklearn.metrics.f1_score(y_test, preds, average='weighted')\n",
    "    f1 = sklearn.metrics.f1_score(y_test, preds, average=None)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, preds)\n",
    "    prec = sklearn.metrics.precision_score(y_test,preds, average=None) \n",
    "    rec = sklearn.metrics.recall_score(y_test,preds, average=None)\n",
    "    \n",
    "    if print_output:\n",
    "        print ('weighted f1: ', f1_w)\n",
    "        print ('f1:          ', f1)\n",
    "        print ('accuracy:    ', acc)\n",
    "        print ('precision:   ', prec)\n",
    "        print ('recall:      ', rec)\n",
    "        print ('admission sens: ', rec[0])\n",
    "        print ('admission spec: ', rec[1])\n",
    "            \n",
    "    return f1_w, f1, acc, prec, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JJ_gridsearch(weights):\n",
    "    epoch = 0\n",
    "    for weight in weights:\n",
    "        \n",
    "        xgc = xgb.XGBClassifier(scale_pos_weight=weight)\n",
    "        xgc.fit(X_train, y_train)\n",
    "        preds = xgc.predict(X_test)\n",
    "        f1_w, f1, acc, prec, rec = get_metrics(y_test, preds)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            results_df = pd.DataFrame([epoch+1, 1/weight, f1_w, f1, acc, prec, rec, rec[0], rec[1]]).T\n",
    "            results_df.columns = ['trial number', 'class penalty', 'weighted f1', 'f1', 'accuracy', 'precision', 'recall', 'admission sensitivity', 'admission specificity']\n",
    "        else:\n",
    "            results_df.loc[len(results_df)] = [epoch+1, 1/weight, f1_w, f1, acc, prec, rec, rec[0], rec[1]]\n",
    "        \n",
    "        epoch +=1\n",
    "        \n",
    "    return results_df, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell when working from home\n",
    "path = '/Users/jjaskolkambp/Desktop/machine learning/my_projects/ed-triage'\n",
    "data_path = '/Users/jjaskolkambp/Desktop/machine learning/my_projects/data/ED triage project/combo'\n",
    "model_path = '/Users/jjaskolkambp/Desktop/machine learning/my_projects/ed-triage/models'\n",
    "\n",
    "#embeds = pd.read_csv(data_path + '/subjnote_embeds.csv', index_col = 0,low_memory = False)\n",
    "clin = pd.read_csv(data_path + '/complete_clean_combo_data.csv', index_col = 0,low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "medhx = np.load(data_path + '/medhx_embeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.load(data_path + '/admit_dc_target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(data_path + '/subj_emeds.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sanity checking the outcome variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 149218, 0.0: 16215})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This array is an array of 768 element vectors for each entry into subjective notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['admit', 'discharge'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### first we are going to train xgb without class penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgc = xgb.XGBClassifier()\n",
    "\n",
    "%time xgc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = xgc.predict(X_test)\n",
    "\n",
    "predictions = xgc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1_w, f1, acc, prec, rec = get_metrics(y_test, preds, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, preds, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thresholding_analysis(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now we are going to try with an empiric class penality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgc = xgb.XGBClassifier(scale_pos_weight=1/9)\n",
    "\n",
    "%time xgc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgc.predict(X_test)\n",
    "predictions = xgc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_w, f1, acc, prec, rec = get_metrics(y_test, preds, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, preds, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholding_analysis(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we are going to repeat the process with medical history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### first we are going to train xgb without class penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgc = xgb.XGBClassifier()\n",
    "\n",
    "%time xgc.fit(X_train[:10000], y_train[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = xgc.predict(X_test)\n",
    "\n",
    "predictions = xgc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1_w, f1, acc, prec, rec = get_metrics(y_test, preds, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, preds, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "thresholding_analysis(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now we are going to try with an empiric class penality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(medhx, target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train2 == y_train), set(y_test ==  y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgc2 = xgb.XGBClassifier(scale_pos_weight=1/9)\n",
    "\n",
    "%time xgc2.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = xgc2.predict(X_test2)\n",
    "predictions2 = xgc2.predict_proba(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_w, f1, acc, prec, rec = get_metrics(y_test, preds2, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, preds2, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thresholding_analysis(predictions2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ensembling the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_predictions = (predictions + predictions2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholding_analysis(combined_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgc, open(data_path + \"/subj_hx_xgb.pkl\", \"wb\"))\n",
    "pickle.dump(xgc2, open(data_path + \"/medhx_xgb.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + \"/tabular_xgb.pkl\", mode = 'rb') as pkl:\n",
    "    xgc3 = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to revisit this. gotta figure out how to get all three models trained and testing together.  probably easiest, just to do it all one more time (!) in a new notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
