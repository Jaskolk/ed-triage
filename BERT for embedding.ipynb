{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX_ZDhicpHkV"
   },
   "source": [
    "## Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3527,
     "status": "ok",
     "timestamp": 1571579166766,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "DEfSbAA4QHas",
    "outputId": "e704a3da-cbd3-4b4f-8c3e-26e1362e4d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6934,
     "status": "ok",
     "timestamp": 1571579173304,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "0NmMdkZO8R6q",
    "outputId": "e21a1118-169b-4e87-fe9c-428b8e41391b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 6.7MB/s \n",
      "\u001b[?25hCollecting pytorch-nlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/ae/b6d18c3f37da5a78e83701469e6153811f4b0ecb3f9387bb3e9a65ca48ee/pytorch_nlp-0.4.1-py3-none-any.whl (82kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 22.7MB/s \n",
      "\u001b[?25hCollecting boto3 (from pytorch-pretrained-bert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/fa/6397049020b312f71c397fff8d10247c2e49da760e2807af7d21e3c23695/boto3-1.9.253-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 17.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.15.4)\n",
      "Collecting regex (from pytorch-pretrained-bert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 21.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (from pytorch-nlp) (0.23.4)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->pytorch-pretrained-bert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 21.3MB/s \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.253 (from boto3->pytorch-pretrained-bert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/7b/88f10115b4748f86be6b7b1d8761ba5023fccf6e6cbe762e368f63eddcf9/botocore-1.12.253-py2.py3-none-any.whl (5.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.8MB 6.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/site-packages (from pandas->pytorch-nlp) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/site-packages (from pandas->pytorch-nlp) (2018.7)\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<1.13.0,>=1.12.253->boto3->pytorch-pretrained-bert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K    100% |████████████████████████████████| 552kB 28.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas->pytorch-nlp) (1.12.0)\n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: docutils, jmespath, botocore, s3transfer, boto3, regex, pytorch-pretrained-bert, pytorch-nlp\n",
      "Successfully installed boto3-1.9.253 botocore-1.12.253 docutils-0.15.2 jmespath-0.9.4 pytorch-nlp-0.4.1 pytorch-pretrained-bert-0.6.2 regex-2019.8.19 s3transfer-0.2.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2500,
     "status": "ok",
     "timestamp": 1571579181811,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "Ok002ceNB8E7",
    "outputId": "eb4ff100-bad7-44bd-ff2d-2afd408fde39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqG7FzRVFEIv"
   },
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1012,
     "status": "ok",
     "timestamp": 1571579184010,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "oYsV4H8fCpZ-",
    "outputId": "ce1d5ac4-bac5-4223-f31b-aa9b21bc07b9"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell when working online\n",
    "path = '/floyd/home/ed-triage'\n",
    "data_path = '/floyd/home/data/egh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0HXL90y47tD"
   },
   "source": [
    "## second pass\n",
    "now going to figure out how to run this on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-b_nZYT05Ae3"
   },
   "outputs": [],
   "source": [
    "def create_dummy_column(s):\n",
    "  if str(s) == 'nan':\n",
    "    return 'empty cell'\n",
    "  else:\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1276,
     "status": "ok",
     "timestamp": 1571579219303,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "Bqs-jcTP5Ius",
    "outputId": "fa40609b-c590-4557-cec7-14d466fc769a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85154, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_path + '/subj_data.csv', index_col = 0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZB-FL7K5LBe"
   },
   "outputs": [],
   "source": [
    "data['for embedding'] = data.CleanSubjectiveNotes.map(create_dummy_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGUlz_bo595x"
   },
   "outputs": [],
   "source": [
    "sentences = data['for embedding'].values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37782,
     "status": "ok",
     "timestamp": 1571579393262,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "hCAcGIEr63hQ",
    "outputId": "e1881cda-55ba-43d9-96f4-05721bf21f35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 931811.75B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'empty', 'cell', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35298,
     "status": "ok",
     "timestamp": 1571579393263,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "21dBlHkebaac",
    "outputId": "e6ae4500-fbdc-46f8-c9ab-350477c5b1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show a few other tokenized sentences\n",
      "['[CLS]', 'empty', 'cell', '[SEP]']\n",
      "['[CLS]', 'empty', 'cell', '[SEP]']\n",
      "['[CLS]', 'lt', 'sided', 'chest', 'pain', 'since', '113', '##0', 'hours', 'lasted', 'for', '3', '-', '4', 'minutes', ',', 'pain', 'back', 'again', 'at', '1200', 'hours', '.', 'no', 'short', '##ness', 'of', 'breath', ',', 'no', 'di', '##zziness', '[SEP]']\n",
      "['[CLS]', 'complain', '##s', 'of', 'flank', 'pain', ',', 'advise', 'by', 'dr', '.', 'ts', '##ilia', '##s', 'to', 'come', 'to', 'er', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print ('show a few other tokenized sentences')\n",
    "print (tokenized_texts[10])\n",
    "print (tokenized_texts[100])\n",
    "print (tokenized_texts[1000])\n",
    "print (tokenized_texts[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EvIzO21KuOQ"
   },
   "outputs": [],
   "source": [
    "data['tokenized_subj_notes'] = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6szzlbqO63hV"
   },
   "source": [
    "BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
    "\n",
    "- **input ids**: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
    "- **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
    "- **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
    "- **labels**: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1291,
     "status": "ok",
     "timestamp": 1571579435728,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "9rhnM8oz63hV",
    "outputId": "93b8e93c-67df-4843-af02-f581c8c67c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean sentence length 33.50418066092021\n",
      "max sentence length 216\n"
     ]
    }
   ],
   "source": [
    "sent_lens = [len(x) for x in tokenized_texts]\n",
    "print ('mean sentence length', np.mean(sent_lens))\n",
    "print ('max sentence length', np.max(sent_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LD756WGw63hX"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = np.max(sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b55KVcvi63hb"
   },
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBYrmzqi63hd"
   },
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYsurfRY63he"
   },
   "source": [
    "Create the attention masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXVmnYSc63hf"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXy57DA063hg"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(input_ids)\n",
    "train_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b18X9iAT63hh"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29010,
     "status": "ok",
     "timestamp": 1571579502159,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "4iLZ41QQ63hk",
    "outputId": "00ec8ddf-cc3f-4ee9-ffce-23d60f0741ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:14<00:00, 27588936.43B/s]\n"
     ]
    }
   ],
   "source": [
    "#we are going to download the model and transfer it to cuda\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QVYAX-Rszk3N"
   },
   "source": [
    "## sentence embedding below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ZSKQNxulb07u",
    "outputId": "6a3010de-7f6e-4d0c-8b24-abdb2ed78db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch #: 1\n",
      "results stored...\n",
      "batch #: 501\n",
      "results stored...\n",
      "batch #: 1001\n",
      "results stored...\n"
     ]
    }
   ],
   "source": [
    "#gonna redo, but this time only keep the sentences\n",
    "model.eval()\n",
    "embedded_notes = []\n",
    "for i, x in enumerate(train_dataloader):\n",
    "  \n",
    "  inpseq = x[0].to(device)\n",
    "  inpmask = x[1].to(device)\n",
    "  embeds,pooled = model(inpseq, attention_mask = inpmask)\n",
    "  sentence_vec = torch.mean(embeds[11],1)  #this is supposedly where the sentences are\n",
    "  #print (sentence_vec.shape)\n",
    "  embedded_notes.append(sentence_vec.cpu().detach().numpy())\n",
    "  if i%500 == 0:\n",
    "    print ('batch #:', i+1)\n",
    "    #print('input sequence shape:', x[0].shape)\n",
    "    #print ('mask shape:', x[1].shape)\n",
    "    print ('results stored...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1571518359821,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "i3u7sMImcsRg",
    "outputId": "78103e7a-81d3-4122-89f3-556449a90c64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5323"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxLojYYzdosp"
   },
   "outputs": [],
   "source": [
    "flat_sentences = [item for sublist in embedded_notes for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-sis2hQz2Xw"
   },
   "outputs": [],
   "source": [
    "data['temp'] = flat_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAQpCOVSz8XW"
   },
   "outputs": [],
   "source": [
    "data['embedded_subnotes'] = data['temp'][data['CleanSubjectiveNotes'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1571518968809,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "WihDHnpG1FF8",
    "outputId": "683b43fc-1f61-4054-bb45-ab3e2f56a9a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanSubjectiveNotes</th>\n",
       "      <th>MedicalHistory</th>\n",
       "      <th>pmhx</th>\n",
       "      <th>tokenized_subj_notes</th>\n",
       "      <th>pooled</th>\n",
       "      <th>embedded_sentences</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.81541735, -0.21443638, -0.17744659, 0.6907...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.9214988, -0.31617814, -0.4026264, 0.831041...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.76925135, -0.16731277, 0.63148165, 0.60955...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.8026976, -0.20626362, 0.33342388, 0.616272...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.9585385, -0.45988795, -0.8535837, 0.861542...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.8474464, -0.28719974, -0.3201902, 0.689953...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.8720149, -0.2216901, 0.31625006, 0.7715695...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.7794212, -0.12808736, 0.60340405, 0.635327...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.78633195, -0.18919525, 0.3827547, 0.634590...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.75231934, -0.21003105, 0.4703341, 0.559403...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.8588793, -0.30978727, -0.44043842, 0.70403...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.5882745, -0.13477229, 0.5233164, 0.3716030...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.83676857, -0.27938357, 0.19051197, 0.74465...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.8643194, -0.32749596, -0.38440666, 0.72149...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.9027696, -0.421154, -0.7414986, 0.79079497...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.9271007, -0.28747287, 0.43418172, 0.659964...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.90086484, -0.22871435, -0.21822777, 0.7749...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.77976644, -0.12196798, 0.17518307, 0.58827...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.68655443, -0.13905688, 0.55926263, 0.49178...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.90440637, -0.2379817, 0.1430055, 0.8072412...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CleanSubjectiveNotes  ...  new\n",
       "0                   NaN  ...  NaN\n",
       "1                   NaN  ...  NaN\n",
       "2                   NaN  ...  NaN\n",
       "3                   NaN  ...  NaN\n",
       "4                   NaN  ...  NaN\n",
       "5                   NaN  ...  NaN\n",
       "6                   NaN  ...  NaN\n",
       "7                   NaN  ...  NaN\n",
       "8                   NaN  ...  NaN\n",
       "9                   NaN  ...  NaN\n",
       "10                  NaN  ...  NaN\n",
       "11                  NaN  ...  NaN\n",
       "12                  NaN  ...  NaN\n",
       "13                  NaN  ...  NaN\n",
       "14                  NaN  ...  NaN\n",
       "15                  NaN  ...  NaN\n",
       "16                  NaN  ...  NaN\n",
       "17                  NaN  ...  NaN\n",
       "18                  NaN  ...  NaN\n",
       "19                  NaN  ...  NaN\n",
       "\n",
       "[20 rows x 7 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1571518990956,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "ZKghAshd1HA3",
    "outputId": "d3ef7999-1336-4e51-b048-7c6032c37515"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanSubjectiveNotes</th>\n",
       "      <th>MedicalHistory</th>\n",
       "      <th>pmhx</th>\n",
       "      <th>tokenized_subj_notes</th>\n",
       "      <th>pooled</th>\n",
       "      <th>embedded_sentences</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>intermittent abdominal pain since last night. ...</td>\n",
       "      <td>^c^^^^ctasMEHPL=No Significant Medical History</td>\n",
       "      <td>no significant medical history</td>\n",
       "      <td>[CLS] intermittent abdominal pain since last n...</td>\n",
       "      <td>[-0.7541863, -0.38550034, -0.935306, 0.6506193...</td>\n",
       "      <td>[-0.17362829, -0.1274244, 0.38404685, -0.16352...</td>\n",
       "      <td>[-0.17362829, -0.1274244, 0.38404685, -0.16352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>for suture removal  rt thumb.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[CLS] for suture removal  rt thumb. [SEP]</td>\n",
       "      <td>[-0.7758408, -0.26031324, -0.6724933, 0.686550...</td>\n",
       "      <td>[-0.28641352, -0.27055612, 0.48553735, -0.2155...</td>\n",
       "      <td>[-0.28641352, -0.27055612, 0.48553735, -0.2155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>was seen at  mackenzie health last sunday afte...</td>\n",
       "      <td>^c^^^^ctasMEHPL=HTN, NIDDM,</td>\n",
       "      <td>htn, niddm</td>\n",
       "      <td>[CLS] was seen at  mackenzie health last sunda...</td>\n",
       "      <td>[-0.8877131, -0.40006533, -0.941798, 0.8546101...</td>\n",
       "      <td>[-0.025925823, -0.16426979, 0.42920288, -0.115...</td>\n",
       "      <td>[-0.025925823, -0.16426979, 0.42920288, -0.115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>pain across the chest non radiating  x3 days, ...</td>\n",
       "      <td>^c^^^^ctasMEHPL=Niddm, cardiac x2, high choles...</td>\n",
       "      <td>niddm, cardiac x2, high cholesterol, htn, anxi...</td>\n",
       "      <td>[CLS] pain across the chest non radiating  x3 ...</td>\n",
       "      <td>[-0.75477433, -0.38048938, -0.9076633, 0.62457...</td>\n",
       "      <td>[-0.13821964, 0.1634989, 0.31855455, -0.376983...</td>\n",
       "      <td>[-0.13821964, 0.1634989, 0.31855455, -0.376983...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>^c^^^^ctasMEHPL=htn, gerd, skin lesion to head</td>\n",
       "      <td>htn, gerd, skin lesion to head</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.9424013, -0.3451816, 0.33916035, 0.7134917...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20005</th>\n",
       "      <td>kicked by resident at the nh 3 days ago compla...</td>\n",
       "      <td>^c^^^^ctasMEHPL=appendectomy, abd hernia, asthma,</td>\n",
       "      <td>appendectomy, abd hernia, asthma</td>\n",
       "      <td>[CLS] kicked by resident at the nh 3 days ago ...</td>\n",
       "      <td>[-0.766297, -0.15209605, -0.69855046, 0.506783...</td>\n",
       "      <td>[-0.13807704, -0.10910831, 0.17822601, -0.2196...</td>\n",
       "      <td>[-0.13807704, -0.10910831, 0.17822601, -0.2196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20006</th>\n",
       "      <td>left arm/shoulder pain x 2 weeks.  was seen by...</td>\n",
       "      <td>^c^^^^ctasMEHPL=NIDDM, gerd,HTN, high cholesterol</td>\n",
       "      <td>niddm, gerd, htn, high cholesterol</td>\n",
       "      <td>[CLS] left arm/shoulder pain x 2 weeks.  was s...</td>\n",
       "      <td>[-0.72606647, -0.3469878, -0.98886853, 0.87553...</td>\n",
       "      <td>[-0.2114657, 0.110205136, 0.45870924, -0.23148...</td>\n",
       "      <td>[-0.2114657, 0.110205136, 0.45870924, -0.23148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20007</th>\n",
       "      <td>diagnosed with psoriasis 10 years ago, skin ra...</td>\n",
       "      <td>^c^^^^ctasMEHPL=psoriasis, htn, high cholester...</td>\n",
       "      <td>psoriasis, htn, high cholesterol, niddm</td>\n",
       "      <td>[CLS] diagnosed with psoriasis 10 years ago, s...</td>\n",
       "      <td>[-0.78073835, -0.42687625, -0.91565096, 0.6277...</td>\n",
       "      <td>[-0.0015239774, -0.007246774, 0.15447551, -0.2...</td>\n",
       "      <td>[-0.0015239774, -0.007246774, 0.15447551, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20008</th>\n",
       "      <td>generalized weakness x 2 weeks.has an appointm...</td>\n",
       "      <td>^c^^^^ctasMEHPL=niddm, HIGH CHOLESTEROL</td>\n",
       "      <td>niddm, high cholesterol</td>\n",
       "      <td>[CLS] generalized weakness x 2 weeks.has an ap...</td>\n",
       "      <td>[-0.9108423, -0.4612296, -0.9929832, 0.9246962...</td>\n",
       "      <td>[-0.24177133, 0.04832459, 0.29758218, -0.29893...</td>\n",
       "      <td>[-0.24177133, 0.04832459, 0.29758218, -0.29893...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20009</th>\n",
       "      <td>on and off mid abdominal pain x3 weeks,  more ...</td>\n",
       "      <td>^c^^^^ctasMEHPL=abdominal hernia,</td>\n",
       "      <td>abdominal hernia</td>\n",
       "      <td>[CLS] on and off mid abdominal pain x3 weeks, ...</td>\n",
       "      <td>[-0.7534101, -0.35356155, -0.94269687, 0.73001...</td>\n",
       "      <td>[-0.10759002, -0.19900884, 0.3312311, -0.28770...</td>\n",
       "      <td>[-0.10759002, -0.19900884, 0.3312311, -0.28770...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20010</th>\n",
       "      <td>pt was struck by a car.pt fell and sustained i...</td>\n",
       "      <td>^c^^^^ctasMEHPL=No Significant Medical History</td>\n",
       "      <td>no significant medical history</td>\n",
       "      <td>[CLS] pt was struck by a car.pt fell and susta...</td>\n",
       "      <td>[-0.68703437, -0.45085445, -0.9937556, 0.85742...</td>\n",
       "      <td>[0.064786784, -0.13853918, 0.4493658, 0.103969...</td>\n",
       "      <td>[0.064786784, -0.13853918, 0.4493658, 0.103969...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20011</th>\n",
       "      <td>diagnosed with lt hand cellulitis yesterday, n...</td>\n",
       "      <td>^c^^^^ctasMEHPL=No Significant Medical History</td>\n",
       "      <td>no significant medical history</td>\n",
       "      <td>[CLS] diagnosed with lt hand cellulitis yester...</td>\n",
       "      <td>[-0.7667637, -0.5217111, -0.9572499, 0.7360626...</td>\n",
       "      <td>[-0.1334282, -0.19342028, 0.45700917, -0.22697...</td>\n",
       "      <td>[-0.1334282, -0.19342028, 0.45700917, -0.22697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>twisted left ankle last night while walking.  ...</td>\n",
       "      <td>^c^^^^ctasMEHPL=No Significant Medical History</td>\n",
       "      <td>no significant medical history</td>\n",
       "      <td>[CLS] twisted left ankle last night while walk...</td>\n",
       "      <td>[-0.89408004, -0.48961493, -0.98943615, 0.8812...</td>\n",
       "      <td>[-0.22627915, -0.14049692, 0.30223975, -0.2746...</td>\n",
       "      <td>[-0.22627915, -0.14049692, 0.30223975, -0.2746...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20013</th>\n",
       "      <td>intermittent  lt chest pain radiating to neck ...</td>\n",
       "      <td>^c^^^^ctasMEHPL=anxiety</td>\n",
       "      <td>anxiety</td>\n",
       "      <td>[CLS] intermittent  lt chest pain radiating to...</td>\n",
       "      <td>[-0.61595654, -0.14701433, -0.6949433, 0.47431...</td>\n",
       "      <td>[-0.09224629, -0.0875343, 0.21740532, -0.17181...</td>\n",
       "      <td>[-0.09224629, -0.0875343, 0.21740532, -0.17181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20014</th>\n",
       "      <td>dizziness x 3 weeks. was seen by fd for the sa...</td>\n",
       "      <td>^c^^^^ctasMEHPL=vertigo,htn</td>\n",
       "      <td>vertigo, htn</td>\n",
       "      <td>[CLS] dizziness x 3 weeks. was seen by fd for ...</td>\n",
       "      <td>[-0.7957736, -0.19073154, -0.9343586, 0.736478...</td>\n",
       "      <td>[-0.27797037, 0.089653425, 0.35464, -0.2019927...</td>\n",
       "      <td>[-0.27797037, 0.089653425, 0.35464, -0.2019927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20015</th>\n",
       "      <td>left knee/leg pain x  5 months. was seen by fd...</td>\n",
       "      <td>^c^^^^ctasMEHPL=Hypertension (HTN)</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>[CLS] left knee/leg pain x  5 months. was seen...</td>\n",
       "      <td>[-0.7643201, -0.29871342, -0.9665053, 0.827076...</td>\n",
       "      <td>[-0.33341393, -0.06526986, 0.43999767, -0.1905...</td>\n",
       "      <td>[-0.33341393, -0.06526986, 0.43999767, -0.1905...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20016</th>\n",
       "      <td>c/0 chest tightness, shortness of breath and h...</td>\n",
       "      <td>^c^^^^ctasMEHPL=No Significant Medical History</td>\n",
       "      <td>no significant medical history</td>\n",
       "      <td>[CLS] c/0 chest tightness, shortness of breath...</td>\n",
       "      <td>[-0.7257751, -0.34747517, -0.8220458, 0.566371...</td>\n",
       "      <td>[-0.1611683, 0.0028105245, 0.4467453, -0.25237...</td>\n",
       "      <td>[-0.1611683, 0.0028105245, 0.4467453, -0.25237...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>^c^^^^ctasMEHPL=CVA with Rt sided deficit,</td>\n",
       "      <td>cva with rt sided deficit</td>\n",
       "      <td>[CLS] empty cell [SEP]</td>\n",
       "      <td>[-0.9209912, -0.2338024, 0.80994904, 0.7390131...</td>\n",
       "      <td>[0.11423147, 0.051171873, 0.01352569, 0.083628...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20018</th>\n",
       "      <td>shortness of breath since this morning,  with ...</td>\n",
       "      <td>^c^^^^ctasMEHPL=No Significant Medical History</td>\n",
       "      <td>no significant medical history</td>\n",
       "      <td>[CLS] shortness of breath since this morning, ...</td>\n",
       "      <td>[-0.8794438, -0.44652268, -0.95824194, 0.80391...</td>\n",
       "      <td>[-0.12998196, -0.10295084, 0.20119476, -0.3051...</td>\n",
       "      <td>[-0.12998196, -0.10295084, 0.20119476, -0.3051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20019</th>\n",
       "      <td>on and off pain back of the head x6 months. se...</td>\n",
       "      <td>^c^^^^ctasMEHPL=hypothyroidism,</td>\n",
       "      <td>hypothyroidism</td>\n",
       "      <td>[CLS] on and off pain back of the head x6 mont...</td>\n",
       "      <td>[-0.6863903, -0.36898285, -0.95912266, 0.72168...</td>\n",
       "      <td>[-0.16572538, -0.08796324, 0.61395025, -0.3059...</td>\n",
       "      <td>[-0.16572538, -0.08796324, 0.61395025, -0.3059...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    CleanSubjectiveNotes  ...                                                new\n",
       "20000  intermittent abdominal pain since last night. ...  ...  [-0.17362829, -0.1274244, 0.38404685, -0.16352...\n",
       "20001                      for suture removal  rt thumb.  ...  [-0.28641352, -0.27055612, 0.48553735, -0.2155...\n",
       "20002  was seen at  mackenzie health last sunday afte...  ...  [-0.025925823, -0.16426979, 0.42920288, -0.115...\n",
       "20003  pain across the chest non radiating  x3 days, ...  ...  [-0.13821964, 0.1634989, 0.31855455, -0.376983...\n",
       "20004                                                NaN  ...                                                NaN\n",
       "20005  kicked by resident at the nh 3 days ago compla...  ...  [-0.13807704, -0.10910831, 0.17822601, -0.2196...\n",
       "20006  left arm/shoulder pain x 2 weeks.  was seen by...  ...  [-0.2114657, 0.110205136, 0.45870924, -0.23148...\n",
       "20007  diagnosed with psoriasis 10 years ago, skin ra...  ...  [-0.0015239774, -0.007246774, 0.15447551, -0.2...\n",
       "20008  generalized weakness x 2 weeks.has an appointm...  ...  [-0.24177133, 0.04832459, 0.29758218, -0.29893...\n",
       "20009  on and off mid abdominal pain x3 weeks,  more ...  ...  [-0.10759002, -0.19900884, 0.3312311, -0.28770...\n",
       "20010  pt was struck by a car.pt fell and sustained i...  ...  [0.064786784, -0.13853918, 0.4493658, 0.103969...\n",
       "20011  diagnosed with lt hand cellulitis yesterday, n...  ...  [-0.1334282, -0.19342028, 0.45700917, -0.22697...\n",
       "20012  twisted left ankle last night while walking.  ...  ...  [-0.22627915, -0.14049692, 0.30223975, -0.2746...\n",
       "20013  intermittent  lt chest pain radiating to neck ...  ...  [-0.09224629, -0.0875343, 0.21740532, -0.17181...\n",
       "20014  dizziness x 3 weeks. was seen by fd for the sa...  ...  [-0.27797037, 0.089653425, 0.35464, -0.2019927...\n",
       "20015  left knee/leg pain x  5 months. was seen by fd...  ...  [-0.33341393, -0.06526986, 0.43999767, -0.1905...\n",
       "20016  c/0 chest tightness, shortness of breath and h...  ...  [-0.1611683, 0.0028105245, 0.4467453, -0.25237...\n",
       "20017                                                NaN  ...                                                NaN\n",
       "20018  shortness of breath since this morning,  with ...  ...  [-0.12998196, -0.10295084, 0.20119476, -0.3051...\n",
       "20019  on and off pain back of the head x6 months. se...  ...  [-0.16572538, -0.08796324, 0.61395025, -0.3059...\n",
       "\n",
       "[20 rows x 7 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[20000:20010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 534895,
     "status": "ok",
     "timestamp": 1571519625421,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "9zVd1f7H1MbA",
    "outputId": "8c800d7b-a9ee-4159-d006-59c677825827"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['embedded_subnotes'].to_csv('/content/subjnote_embeds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmqSACnc2Vlm"
   },
   "source": [
    "## now gonna sentence embed medical history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOzGihsu2p8n"
   },
   "outputs": [],
   "source": [
    "#need an way to handle empty cells so i can still embed the whole dataset and plug it back into the dataframe\n",
    "data['for embedding'] = data['pmhx'].map(create_dummy_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Mk50kP32p8q"
   },
   "outputs": [],
   "source": [
    "sentences = data['for embedding'].values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10905,
     "status": "ok",
     "timestamp": 1571519816776,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "udl407md2p8s",
    "outputId": "51d206c3-dffa-479e-bbbc-bbc1bde98e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'empty', 'cell', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9353,
     "status": "ok",
     "timestamp": 1571519816778,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "olUheiYj4NlB",
    "outputId": "7e12748d-d23d-41cf-d3c6-1150015391fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'empty', 'cell', '[SEP]']\n",
      "['[CLS]', 'empty', 'cell', '[SEP]']\n",
      "['[CLS]', 'no', 'significant', 'medical', 'history', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print ('show a few other tokenized sentences')\n",
    "print (tokenized_texts[10])\n",
    "print (tokenized_texts[100])\n",
    "print (tokenized_texts[1000])\n",
    "print (tokenized_texts[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VI5yqO0B2p8u"
   },
   "outputs": [],
   "source": [
    "data['tokenized_medhx'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1571519827354,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "KdSlUgJz2p8z",
    "outputId": "f05b6b3b-876a-4e33-95f8-a2ad89734dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean sentence length 7.951229537074007\n",
      "max sentence length 101\n"
     ]
    }
   ],
   "source": [
    "sent_lens = [len(x) for x in tokenized_texts]\n",
    "print ('mean sentence length:', np.mean(sent_lens))\n",
    "print ('max sentence length:', np.max(sent_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C50RE0Dm2p81"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = np.max(sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkCSa_Bc2p84"
   },
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQaMAuia2p86"
   },
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMjxtTBA2p87"
   },
   "source": [
    "Create the attention masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oM9YlQH2p88"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryorOZKh2p89"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(input_ids)\n",
    "train_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ff4rOgd22p8_"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUwMVWFk2p9B"
   },
   "outputs": [],
   "source": [
    "#we are going to download the model and transfer it to cuda\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1093853,
     "status": "ok",
     "timestamp": 1571520969916,
     "user": {
      "displayName": "Jeff Jaskolka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAxPoaD5czR7gp1OnGR14wAKPoLvTCIdkN9Bp5MTg=s64",
      "userId": "18090659719286838097"
     },
     "user_tz": 240
    },
    "id": "2lOAiTVR3Lli",
    "outputId": "49aaa2ba-791a-40b6-9215-ec7ef1e101f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch #: 1\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 51\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 101\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 151\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 201\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 251\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 301\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 351\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 401\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 451\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 501\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 551\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 601\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 651\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 701\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 751\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 801\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 851\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 901\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 951\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1001\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1051\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1101\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1151\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1201\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1251\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1301\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1351\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1401\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1451\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1501\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1551\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1601\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1651\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1701\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1751\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1801\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1851\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1901\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 1951\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2001\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2051\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2101\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2151\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2201\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2251\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2301\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2351\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2401\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2451\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2501\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2551\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2601\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2651\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2701\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2751\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2801\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2851\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2901\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 2951\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3001\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3051\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3101\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3151\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3201\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3251\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3301\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3351\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3401\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3451\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3501\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3551\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3601\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3651\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3701\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3751\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3801\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3851\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3901\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 3951\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4001\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4051\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4101\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4151\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4201\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4251\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4301\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4351\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4401\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4451\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4501\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4551\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4601\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4651\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4701\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4751\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4801\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4851\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4901\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 4951\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 5001\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 5051\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 5101\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 5151\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 5201\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 5251\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n",
      "batch #: 5301\n",
      "input sequence shape: torch.Size([16, 101])\n",
      "mask shape: torch.Size([16, 101])\n",
      "results stored...\n"
     ]
    }
   ],
   "source": [
    "#gonna redo, but this time only keep the sentences\n",
    "model.eval()\n",
    "embedded_notes = []\n",
    "for i, x in enumerate(train_dataloader):\n",
    "  \n",
    "  inpseq = x[0].to(device)\n",
    "  inpmask = x[1].to(device)\n",
    "  embeds,pooled = model(inpseq, attention_mask = inpmask)\n",
    "  sentence_vec = torch.mean(embeds[11],1)  #this is supposedly where the sentences are\n",
    "  #print (sentence_vec.shape)\n",
    "  embedded_notes.append(sentence_vec.cpu().detach().numpy())\n",
    "  if i%500 == 0:\n",
    "    print ('batch #:', i+1)\n",
    "    #print('input sequence shape:', x[0].shape)\n",
    "    #print ('mask shape:', x[1].shape)\n",
    "    print ('results stored...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3b60bU073chl"
   },
   "outputs": [],
   "source": [
    "data['new'] = data['embedded_sentences'][data['CleanSubjectiveNotes'].notnull() ]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BERT for embedding.ipynb",
   "provenance": [
    {
     "file_id": "10SI-59huCzMvDOlVJZZRNZ0trAZeIj9g",
     "timestamp": 1571430967692
    },
    {
     "file_id": "1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO",
     "timestamp": 1570654919298
    },
    {
     "file_id": "1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y",
     "timestamp": 1556493831452
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
