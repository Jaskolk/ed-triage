{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## first pass pre made BERT embeddings with continuous variable\n",
    "- made up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "subj = torch.rand(160,768).numpy()\n",
    "medhx = torch.rand(160,768).numpy()\n",
    "cont = torch.rand(160,10).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels = torch.randint(0,3,(160,)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 768), (160, 768), (160, 10), (160,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj.shape, medhx.shape, cont.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_subj, validation_subj = train_test_split(subj, random_state = 42, test_size=0.1)\n",
    "train_medhx, validation_medhx = train_test_split(medhx, random_state = 42, test_size=0.1)\n",
    "train_cont, validation_cont = train_test_split(cont, random_state = 42, test_size=0.1)\n",
    "train_labels, validation_labels = train_test_split(labels, random_state = 42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_subj = torch.tensor(train_subj)\n",
    "validation_subj = torch.tensor(validation_subj)\n",
    "train_medhx = torch.tensor(train_medhx)\n",
    "validation_medhx = torch.tensor(validation_medhx)\n",
    "train_cont = torch.tensor(train_cont)\n",
    "validation_cont = torch.tensor(validation_cont)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 4\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_subj, train_medhx, train_cont,train_labels)\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(validation_subj, validation_medhx, validation_cont, validation_labels)\n",
    "validloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "lr = 1\n",
    "epochs = 10\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear_subjective = nn.Linear(768,10)  \n",
    "        self.linear_medhx = nn.Linear(768, 10)\n",
    "        self.linear_combined = nn.Linear(30, 10)\n",
    "        self.out = nn.Linear(10,4)\n",
    "\n",
    "    def forward(self, subj,medhx,cont):\n",
    "        nlp1 = F.relu(self.linear_subjective(subj))\n",
    "        nlp2 = F.relu(self.linear_medhx(medhx))\n",
    "        combined = torch.cat((nlp1,nlp2,cont), axis = 1)\n",
    "        x = F.relu(self.linear_combined(combined))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear_subjective): Linear(in_features=768, out_features=10, bias=True)\n",
       "  (linear_medhx): Linear(in_features=768, out_features=10, bias=True)\n",
       "  (linear_combined): Linear(in_features=30, out_features=10, bias=True)\n",
       "  (out): Linear(in_features=10, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### testing the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x1,x2,x3,x4 = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3076,  0.1748, -0.2668, -0.0997],\n",
       "        [ 0.4326,  0.1828, -0.1671, -0.1729],\n",
       "        [ 0.2816,  0.0664, -0.3529, -0.0144],\n",
       "        [ 0.3620,  0.0901, -0.2825, -0.0466]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x1,x2,x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### trying the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    optimizer = Adam(model.parameters(), lr = lr)\n",
    "    running_loss = []\n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step_num, batch_data in enumerate(trainloader):\n",
    "        \n",
    "            cont_var, subj_notes, medhx, labels = tuple(t.to(device) for t in batch_data)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            logits = model(cont_var, subj_notes, medhx)#, cat_var)\n",
    "        \n",
    "            batch_loss = loss_func(logits, labels)\n",
    "        \n",
    "            train_loss += batch_loss.item()\n",
    "        \n",
    "            batch_loss.backward()\n",
    "        \n",
    "\n",
    "            clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            if step_num %12 == 0:\n",
    "                print('Epoch: ', epoch_num + 1)\n",
    "                print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / batch_size, train_loss / (step_num + 1)))\n",
    "            \n",
    "        running_loss.append(train_loss)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/36.0 loss: 1.3358349800109863 \n",
      "Epoch:  1\n",
      "12/36.0 loss: 985.2547945426061 \n",
      "Epoch:  1\n",
      "24/36.0 loss: 513.0024306845665 \n",
      "Epoch:  2\n",
      "0/36.0 loss: 1.128175139427185 \n",
      "Epoch:  2\n",
      "12/36.0 loss: 1.1653862045361445 \n",
      "Epoch:  2\n",
      "24/36.0 loss: 1.2015890789031982 \n",
      "Epoch:  3\n",
      "0/36.0 loss: 1.0959442853927612 \n",
      "Epoch:  3\n",
      "12/36.0 loss: 1.137764884875371 \n",
      "Epoch:  3\n",
      "24/36.0 loss: 1.1805632257461547 \n",
      "Epoch:  4\n",
      "0/36.0 loss: 1.0929811000823975 \n",
      "Epoch:  4\n",
      "12/36.0 loss: 1.1317130418924184 \n",
      "Epoch:  4\n",
      "24/36.0 loss: 1.1769244384765625 \n",
      "Epoch:  5\n",
      "0/36.0 loss: 1.0962785482406616 \n",
      "Epoch:  5\n",
      "12/36.0 loss: 1.1305855329220111 \n",
      "Epoch:  5\n",
      "24/36.0 loss: 1.1765617084503175 \n",
      "Epoch:  6\n",
      "0/36.0 loss: 1.098321795463562 \n",
      "Epoch:  6\n",
      "12/36.0 loss: 1.1301328264749968 \n",
      "Epoch:  6\n",
      "24/36.0 loss: 1.1764910960197448 \n",
      "Epoch:  7\n",
      "0/36.0 loss: 1.099420428276062 \n",
      "Epoch:  7\n",
      "12/36.0 loss: 1.1297948818940382 \n",
      "Epoch:  7\n",
      "24/36.0 loss: 1.1764164423942567 \n",
      "Epoch:  8\n",
      "0/36.0 loss: 1.1000910997390747 \n",
      "Epoch:  8\n",
      "12/36.0 loss: 1.1295231168086712 \n",
      "Epoch:  8\n",
      "24/36.0 loss: 1.1763401889801026 \n",
      "Epoch:  9\n",
      "0/36.0 loss: 1.100547194480896 \n",
      "Epoch:  9\n",
      "12/36.0 loss: 1.129305871633383 \n",
      "Epoch:  9\n",
      "24/36.0 loss: 1.176272325515747 \n",
      "Epoch:  10\n",
      "0/36.0 loss: 1.1008776426315308 \n",
      "Epoch:  10\n",
      "12/36.0 loss: 1.1291310007755573 \n",
      "Epoch:  10\n",
      "24/36.0 loss: 1.1762146306037904 \n"
     ]
    }
   ],
   "source": [
    "cum_loss = train_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12838.60406333208,\n",
       " 42.982205629348755,\n",
       " 42.386277198791504,\n",
       " 42.31040298938751,\n",
       " 42.314825773239136,\n",
       " 42.3207705616951,\n",
       " 42.32360535860062,\n",
       " 42.32485604286194,\n",
       " 42.32542860507965,\n",
       " 42.32569521665573]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2a2cb128>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cum_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we are going to make the model more complex and add embedding\n",
    "- still made up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = np.random.randint(1, 30, size=(160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not sure which the best method for embedding filters is but likley # 2\n",
    "embedding_size = min(50, (len(set(cats))// 2) +1)\n",
    "embedding_size2 = min(600, round(1.6 * len(set(cats))**0.56))\n",
    "embedding_size, embedding_size2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj, validation_subj = train_test_split(subj, random_state = 42, test_size=0.1)\n",
    "train_medhx, validation_medhx = train_test_split(medhx, random_state = 42, test_size=0.1)\n",
    "train_cont, validation_cont = train_test_split(cont, random_state = 42, test_size=0.1)\n",
    "train_cats, validation_cats = train_test_split(cats, random_state = 42, test_size=0.1)\n",
    "train_labels, validation_labels = train_test_split(labels, random_state = 42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj = torch.tensor(train_subj)\n",
    "validation_subj = torch.tensor(validation_subj)\n",
    "train_medhx = torch.tensor(train_medhx)\n",
    "validation_medhx = torch.tensor(validation_medhx)\n",
    "train_cont = torch.tensor(train_cont)\n",
    "validation_cont = torch.tensor(validation_cont)\n",
    "train_cats = torch.tensor(train_cats)\n",
    "validation_cats = torch.tensor(validation_cats)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 4\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_subj, train_medhx, train_cont,train_cats,train_labels)\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(validation_subj, validation_medhx, validation_cont, validation_cats, validation_labels)\n",
    "validloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.linear_subjective = nn.Linear(768,10)  \n",
    "        self.linear_medhx = nn.Linear(768, 10)\n",
    "        self.embedding = nn.Embedding(len(set(cats))+1, 30)\n",
    "        self.linear_cat = nn.Linear(30,10)\n",
    "        self.linear_combined = nn.Linear(40, 10)\n",
    "        self.out = nn.Linear(10,4)\n",
    "\n",
    "    def forward(self, subj,medhx,cont,cat_var):\n",
    "        nlp1 = F.relu(self.linear_subjective(subj))\n",
    "        nlp2 = F.relu(self.linear_medhx(medhx))\n",
    "        embeds = self.embedding(cat_var)\n",
    "        cats = F.relu(self.linear_cat(embeds))\n",
    "        combined = torch.cat((nlp1,nlp2,cont,cats), axis = 1)\n",
    "        x = F.relu(self.linear_combined(combined))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model2(model):\n",
    "    optimizer = Adam(model.parameters(), lr = lr)\n",
    "    running_loss = []\n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step_num, batch_data in enumerate(trainloader):\n",
    "        \n",
    "            cont_var, subj_notes, medhx, cat_var, labels = tuple(t.to(device) for t in batch_data)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            logits = model(cont_var, subj_notes, medhx, cat_var)\n",
    "        \n",
    "            batch_loss = loss_func(logits, labels)\n",
    "        \n",
    "            train_loss += batch_loss.item()\n",
    "        \n",
    "            batch_loss.backward()\n",
    "        \n",
    "\n",
    "            clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            if step_num %12 == 0:\n",
    "                print('Epoch: ', epoch_num + 1)\n",
    "                print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / batch_size, train_loss / (step_num + 1)))\n",
    "            \n",
    "        running_loss.append(train_loss)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net1(\n",
      "  (linear_subjective): Linear(in_features=768, out_features=10, bias=True)\n",
      "  (linear_medhx): Linear(in_features=768, out_features=10, bias=True)\n",
      "  (embedding): Embedding(30, 30)\n",
      "  (linear_cat): Linear(in_features=30, out_features=10, bias=True)\n",
      "  (linear_combined): Linear(in_features=40, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net1()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1,x2,x3,x4,x5 = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1906, -0.1836,  0.3142,  0.2797],\n",
       "        [ 0.2610, -0.1657,  0.2637,  0.3104],\n",
       "        [ 0.2161, -0.1512,  0.3142,  0.2773],\n",
       "        [ 0.2273, -0.1419,  0.2401,  0.3181]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x1,x2,x3,x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/36.0 loss: 1.524527668952942 \n",
      "Epoch:  1\n",
      "12/36.0 loss: 731.978949537644 \n",
      "Epoch:  1\n",
      "24/36.0 loss: 381.2159674453735 \n",
      "Epoch:  2\n",
      "0/36.0 loss: 1.0909812450408936 \n",
      "Epoch:  2\n",
      "12/36.0 loss: 1.1275699551288898 \n",
      "Epoch:  2\n",
      "24/36.0 loss: 1.1708456039428712 \n",
      "Epoch:  3\n",
      "0/36.0 loss: 1.0890642404556274 \n",
      "Epoch:  3\n",
      "12/36.0 loss: 1.1292571792235742 \n",
      "Epoch:  3\n",
      "24/36.0 loss: 1.1776954865455627 \n",
      "Epoch:  4\n",
      "0/36.0 loss: 1.095662236213684 \n",
      "Epoch:  4\n",
      "12/36.0 loss: 1.128738705928509 \n",
      "Epoch:  4\n",
      "24/36.0 loss: 1.177166726589203 \n",
      "Epoch:  5\n",
      "0/36.0 loss: 1.0981080532073975 \n",
      "Epoch:  5\n",
      "12/36.0 loss: 1.12830715913039 \n",
      "Epoch:  5\n",
      "24/36.0 loss: 1.1765507793426513 \n",
      "Epoch:  6\n",
      "0/36.0 loss: 1.0991584062576294 \n",
      "Epoch:  6\n",
      "12/36.0 loss: 1.1282018652329078 \n",
      "Epoch:  6\n",
      "24/36.0 loss: 1.1763272404670715 \n",
      "Epoch:  7\n",
      "0/36.0 loss: 1.0997880697250366 \n",
      "Epoch:  7\n",
      "12/36.0 loss: 1.1281802837665265 \n",
      "Epoch:  7\n",
      "24/36.0 loss: 1.1762333631515502 \n",
      "Epoch:  8\n",
      "0/36.0 loss: 1.1002320051193237 \n",
      "Epoch:  8\n",
      "12/36.0 loss: 1.128169412796314 \n",
      "Epoch:  8\n",
      "24/36.0 loss: 1.176175343990326 \n",
      "Epoch:  9\n",
      "0/36.0 loss: 1.1005643606185913 \n",
      "Epoch:  9\n",
      "12/36.0 loss: 1.1281583217474132 \n",
      "Epoch:  9\n",
      "24/36.0 loss: 1.1761316084861755 \n",
      "Epoch:  10\n",
      "0/36.0 loss: 1.1008212566375732 \n",
      "Epoch:  10\n",
      "12/36.0 loss: 1.1281466667468731 \n",
      "Epoch:  10\n",
      "24/36.0 loss: 1.1760963678359986 \n"
     ]
    }
   ],
   "source": [
    "cum_loss = train_model2(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next I'm going to make a pipeline for converting a list of categories into embedded classes\n",
    "### and turn those into model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['manatee', 'dog', 'giraffe', 'narwhal', 'cat', 'elephant', 'mouse', 'dingo', 'wombat', 'skunk']\n",
    "cat_data = [[animal]* 16 for animal in animals]\n",
    "cat_data = [item for sublist in cat_data for item in sublist]\n",
    "random.shuffle(cat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_to_num = {animal:i for i,animal in enumerate(list(set(cat_data)))}\n",
    "num_to_animal = {i:animal for animal, i in enumerate(list(set(cat_data)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is going to be my fake categorical animal data\n",
    "numerical_cat_data = [animal_to_num[item] for item in cat_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 10)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numerical_cat_data), set(numerical_cat_data), len(set(numerical_cat_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### copied tabular learner code from fast.ai\n",
    "- trying to figure out how to build the model construction pipeline that I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ListSizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-1ed81e2f4b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTabularModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Basic model for tabular data.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n\u001b[1;32m      4\u001b[0m                  emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-1ed81e2f4b1b>\u001b[0m in \u001b[0;36mTabularModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Basic model for tabular data.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n\u001b[0;32m----> 4\u001b[0;31m                  emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ListSizes' is not defined"
     ]
    }
   ],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
    "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(emb_drop)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [self.n_emb + self.n_cont] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn_cont(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(30, 30)\n",
       "  (1): Embedding(20, 20)\n",
       "  (2): Embedding(400, 50)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs = [(30,30), (20,20), (400,50)]\n",
    "embed_layers = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "embed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Embedding(30, 30), Embedding(20, 20), Embedding(400, 50)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_layers = [nn.Embedding(ni, nf) for ni,nf in emb_szs]\n",
    "embed_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_emb = sum(e.embedding_dim for e in embed_layers); n_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layers=[200,100]\n",
    "n_cont = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sizes(layers, out_sz):\n",
    "    return [n_emb + n_cont] + layers + [out_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103, 200, 100, 4]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = get_sizes(layers, 4); sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ReLU(inplace=True), ReLU(inplace=True), None]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]; actns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn_cont(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cats = np.random.randint(0, 29, size=(160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160,), 29)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.shape, len(set(cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast.ai code code deconstructed above\n",
    "#### using that style to create my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so lets say I have 2 columns of categorical data:\n",
    "cat1 = numerical_cat_data #from the \"animal data\" I created above\n",
    "cat2 = cats #the original random number data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = [cat1,cat2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 29]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_lengths = [len(set(cat)) for cat in cat_list]\n",
    "class_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 2)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is an array the categorical data\n",
    "cats_array = np.vstack([np.array(cat) for cat in cat_list]).transpose(); cats_array.shape\n",
    "\n",
    "#this is a code I used during dev to go straight to a tensor\n",
    "#torch_cats = torch.tensor(np.vstack([np.array(cat) for cat in cat_list])).t(); cat3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 1, 2, 3, 6, 8, 5, 7, 1, 5, 5, 6, 8, 4, 0, 5, 0, 4, 9, 5, 5, 9,\n",
       "       7, 1, 5, 6, 6, 9, 2, 9, 3, 9, 0, 2, 8, 0, 2, 4, 6, 6, 2, 6, 9, 0,\n",
       "       8, 4, 0, 2, 2, 3, 5, 1, 9, 7, 0, 0, 5, 1, 3, 3, 3, 5, 0, 0, 8, 4,\n",
       "       6, 8, 5, 8, 2, 9, 1, 3, 2, 4, 2, 7, 9, 3, 1, 7, 9, 9, 4, 3, 5, 3,\n",
       "       6, 9, 0, 4, 1, 3, 0, 7, 8, 1, 9, 9, 4, 8, 0, 2, 0, 5, 2, 7, 7, 4,\n",
       "       3, 4, 8, 8, 2, 1, 2, 0, 2, 8, 3, 7, 4, 9, 7, 7, 7, 4, 1, 3, 7, 3,\n",
       "       5, 6, 4, 6, 6, 6, 6, 1, 5, 4, 5, 6, 7, 8, 9, 2, 4, 3, 1, 8, 6, 1,\n",
       "       7, 1, 0, 8, 8, 1])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8, 15, 10,  3, 24,  0, 27, 21, 15, 18, 20,  3, 24,  6, 12,  8,\n",
       "       21, 26, 11,  2,  5, 11,  9, 16, 26, 20, 11, 19, 28,  5,  1, 22, 16,\n",
       "        6, 16,  6,  3, 10, 13, 19, 28, 17,  6, 11, 17, 18, 20,  7, 24, 10,\n",
       "       15, 14, 10, 22, 22,  2,  2, 24,  2, 14, 28, 24, 23,  8, 11,  4,  9,\n",
       "        0,  5, 13, 24, 28, 25, 11, 12,  6, 15, 24, 23, 28, 11, 13,  4,  0,\n",
       "       12,  6, 11, 24,  2, 19,  6, 24,  5,  2, 11,  3,  9,  3, 24, 27, 14,\n",
       "        7,  5,  1,  7, 15, 19,  8,  2, 26, 21,  2,  7, 11, 25,  5,  8, 22,\n",
       "       16,  2, 25, 14, 27,  3, 11, 22, 28, 19, 27,  7,  3, 27, 15, 10, 16,\n",
       "       17, 12, 20,  0,  8, 26,  6, 13, 17, 20,  4, 10, 13,  1, 24,  8,  9,\n",
       "       26, 16,  7, 27,  1,  3, 24])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_array[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 10]), 7, 10, array([ 7, 10]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_array[0], cats_array[0,0], cats_array[0,1], cats_array[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 6), (29, 11)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a set of embedding sizes for each categorical variable\n",
    "emb_szs = [(length, min(600, round(1.6 * length **0.56))) for length in class_lengths]\n",
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, emb_szs):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        #first create the embedding layers for each of the categorical variables\n",
    "        self.embed_layers = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embed_layers)\n",
    "        \n",
    "        self.linear_subjective = nn.Linear(768,10)  \n",
    "        self.linear_medhx = nn.Linear(768, 10)\n",
    "        self.linear_cat = nn.Linear(n_emb,10)\n",
    "        self.linear_combined = nn.Linear(40, 10)\n",
    "        self.out = nn.Linear(10,4)\n",
    "\n",
    "    def forward(self, subj,medhx,cont,cat_var):\n",
    "        #first we run each BERT vector through a linear layer and activation\n",
    "        nlp1 = F.relu(self.linear_subjective(subj))\n",
    "        nlp2 = F.relu(self.linear_medhx(medhx))\n",
    "        \n",
    "        #now we pass each categorical variable through an embedding layer, combine output and pass through\n",
    "        #a linear layer with activation\n",
    "        embeds = [e(cat_var[:,i]) for i,e in enumerate(self.embed_layers)]\n",
    "        embeds = torch.cat(embeds, 1)\n",
    "        embeds = F.relu(self.linear_cat(embeds))\n",
    "        \n",
    "        #now we combine all four sources of input and pass through a layer with activation\n",
    "        combined = torch.cat((nlp1,nlp2,cont,embeds), axis = 1)\n",
    "        x = F.relu(self.linear_combined(combined))\n",
    "        \n",
    "        #compute logits\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net2(\n",
       "  (embed_layers): ModuleList(\n",
       "    (0): Embedding(10, 6)\n",
       "    (1): Embedding(29, 11)\n",
       "  )\n",
       "  (linear_subjective): Linear(in_features=768, out_features=10, bias=True)\n",
       "  (linear_medhx): Linear(in_features=768, out_features=10, bias=True)\n",
       "  (linear_cat): Linear(in_features=17, out_features=10, bias=True)\n",
       "  (linear_combined): Linear(in_features=40, out_features=10, bias=True)\n",
       "  (out): Linear(in_features=10, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net2(emb_szs)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2084,  0.1449,  0.0720, -0.0200],\n",
       "        [ 0.1421, -0.0287,  0.0036, -0.1177],\n",
       "        [ 0.1205,  0.1090,  0.1325, -0.0288],\n",
       "        [ 0.1303,  0.0080,  0.1028, -0.0990]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(subj[:4]), torch.tensor(medhx[:4]),torch.tensor(cont[:4]), torch_cats[:4] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets create \"datasets, loaders, etc\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj, validation_subj = train_test_split(subj, random_state = 42, test_size=0.1)\n",
    "train_medhx, validation_medhx = train_test_split(medhx, random_state = 42, test_size=0.1)\n",
    "train_cont, validation_cont = train_test_split(cont, random_state = 42, test_size=0.1)\n",
    "train_cats, validation_cats = train_test_split(cats_array, random_state = 42, test_size=0.1)\n",
    "train_labels, validation_labels = train_test_split(labels, random_state = 42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj = torch.tensor(train_subj)\n",
    "validation_subj = torch.tensor(validation_subj)\n",
    "train_medhx = torch.tensor(train_medhx)\n",
    "validation_medhx = torch.tensor(validation_medhx)\n",
    "train_cont = torch.tensor(train_cont)\n",
    "validation_cont = torch.tensor(validation_cont)\n",
    "train_cats = torch.tensor(train_cats)\n",
    "validation_cats = torch.tensor(validation_cats)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_data = TensorDataset(train_subj, train_medhx, train_cont,train_cats,train_labels)\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(validation_subj, validation_medhx, validation_cont, validation_cats, validation_labels)\n",
    "validloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2,x3,x4,x5 = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8454e-01,  2.4995e-02, -9.8717e-02, -4.3934e-02],\n",
       "        [ 2.1038e-01,  1.0868e-01, -1.2368e-04,  1.2354e-02],\n",
       "        [ 1.5581e-01,  3.5455e-02,  7.8640e-02, -1.0391e-01],\n",
       "        [ 1.8058e-01, -8.5037e-03, -8.8060e-02, -4.7531e-02]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x1,x2,x3,x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/36.0 loss: 1.3684231042861938 \n",
      "Epoch:  1\n",
      "12/36.0 loss: 1965.248020199629 \n",
      "Epoch:  1\n",
      "24/36.0 loss: 1022.6059333252907 \n",
      "Epoch:  2\n",
      "0/36.0 loss: 1.1417428255081177 \n",
      "Epoch:  2\n",
      "12/36.0 loss: 1.1635463925508351 \n",
      "Epoch:  2\n",
      "24/36.0 loss: 1.199290497303009 \n",
      "Epoch:  3\n",
      "0/36.0 loss: 1.0997579097747803 \n",
      "Epoch:  3\n",
      "12/36.0 loss: 1.1375006712399995 \n",
      "Epoch:  3\n",
      "24/36.0 loss: 1.1804805517196655 \n",
      "Epoch:  4\n",
      "0/36.0 loss: 1.0941739082336426 \n",
      "Epoch:  4\n",
      "12/36.0 loss: 1.1313554965532744 \n",
      "Epoch:  4\n",
      "24/36.0 loss: 1.1767557120323182 \n",
      "Epoch:  5\n",
      "0/36.0 loss: 1.097043514251709 \n",
      "Epoch:  5\n",
      "12/36.0 loss: 1.130193448983706 \n",
      "Epoch:  5\n",
      "24/36.0 loss: 1.1763296294212342 \n",
      "Epoch:  6\n",
      "0/36.0 loss: 1.0989452600479126 \n",
      "Epoch:  6\n",
      "12/36.0 loss: 1.1297964866344745 \n",
      "Epoch:  6\n",
      "24/36.0 loss: 1.176282720565796 \n",
      "Epoch:  7\n",
      "0/36.0 loss: 1.0999459028244019 \n",
      "Epoch:  7\n",
      "12/36.0 loss: 1.1295135021209717 \n",
      "Epoch:  7\n",
      "24/36.0 loss: 1.1762409138679504 \n",
      "Epoch:  8\n",
      "0/36.0 loss: 1.1005421876907349 \n",
      "Epoch:  8\n",
      "12/36.0 loss: 1.129283451117002 \n",
      "Epoch:  8\n",
      "24/36.0 loss: 1.1761909413337708 \n",
      "Epoch:  9\n",
      "0/36.0 loss: 1.100940465927124 \n",
      "Epoch:  9\n",
      "12/36.0 loss: 1.1290976634392371 \n",
      "Epoch:  9\n",
      "24/36.0 loss: 1.1761431360244752 \n",
      "Epoch:  10\n",
      "0/36.0 loss: 1.101224422454834 \n",
      "Epoch:  10\n",
      "12/36.0 loss: 1.1289473726199224 \n",
      "Epoch:  10\n",
      "24/36.0 loss: 1.176101233959198 \n"
     ]
    }
   ],
   "source": [
    "cum_loss = train_model2(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2ce44978>]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
