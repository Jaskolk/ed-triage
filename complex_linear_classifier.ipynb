{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear_subjective = nn.Linear(768,10)  \n",
    "        self.linear_medhx = nn.Linear(768, 10)\n",
    "        self.combined = nn.Linear(30, 10)\n",
    "        self.out = nn.Linear(10,4)\n",
    "\n",
    "    def forward(self, subj,medhx,cont):\n",
    "        nlp1 = F.relu(self.linear_subjective(subj))\n",
    "        #print (nlp1.shape)\n",
    "        nlp2 = F.relu(self.linear_medhx(medhx))\n",
    "        #print (nlp2.shape)\n",
    "        combined = torch.cat((nlp1,nlp2,cont), axis = 1)\n",
    "        #print (combined.shape)\n",
    "        x = self.combined(combined)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (linear_subjective): Linear(in_features=768, out_features=10, bias=True)\n",
      "  (linear_medhx): Linear(in_features=768, out_features=10, bias=True)\n",
      "  (combined): Linear(in_features=30, out_features=10, bias=True)\n",
      "  (out): Linear(in_features=10, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = torch.rand(160,768).numpy()\n",
    "medhx = torch.rand(160,768).numpy()\n",
    "cont = torch.rand(160,10).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.randint(0,3,(160,)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 768), (160, 768), (160, 10), (160,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj.shape, medhx.shape, cont.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj, validation_subj = train_test_split(subj, random_state = 42, test_size=0.1)\n",
    "train_medhx, validation_medhx = train_test_split(medhx, random_state = 42, test_size=0.1)\n",
    "train_cont, validation_cont = train_test_split(cont, random_state = 42, test_size=0.1)\n",
    "train_labels, validation_labels = train_test_split(labels, random_state = 42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0086,  1.9166, -0.0743,  ..., -1.5197, -0.1110, -1.7344],\n",
       "        [ 1.9272,  1.7760,  0.0819,  ..., -1.6422, -0.1861, -1.5834],\n",
       "        [ 1.7860,  1.9397, -0.1915,  ..., -1.6480, -0.0814, -1.4577],\n",
       "        ...,\n",
       "        [ 1.7268,  1.7883,  0.0732,  ..., -1.3953, -0.1415, -1.5651],\n",
       "        [ 1.7398,  1.9710, -0.2882,  ..., -1.3591, -0.3295, -1.5359],\n",
       "        [ 1.5939,  1.8365, -0.1336,  ..., -1.4517, -0.2982, -1.5540]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(subj),torch.tensor(medhx),torch.tensor(cont))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj = torch.tensor(train_subj)\n",
    "validation_subj = torch.tensor(validation_subj)\n",
    "train_medhx = torch.tensor(train_medhx)\n",
    "validation_medhx = torch.tensor(validation_medhx)\n",
    "train_cont = torch.tensor(train_cont)\n",
    "validation_cont = torch.tensor(validation_cont)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 4\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_subj, train_medhx, train_cont, train_labels)\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(validation_subj, validation_medhx, validation_cont, validation_labels)\n",
    "validloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "lr = 1e-3\n",
    "optimizer = Adam(net.parameters(), lr = lr)\n",
    "epochs = 3\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step_num, batch_data in enumerate(trainloader):\n",
    "        \n",
    "            cont_var, subj_notes, medhx, labels = tuple(t.to(device) for t in batch_data)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            logits = model(cont_var, subj_notes, medhx)\n",
    "        \n",
    "            batch_loss = loss_func(logits, labels)\n",
    "        \n",
    "            train_loss += batch_loss.item()\n",
    "        \n",
    "            batch_loss.backward()\n",
    "        \n",
    "\n",
    "            clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            print('Epoch: ', epoch_num + 1)\n",
    "            print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / batch_size, train_loss / (step_num + 1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "0/36.0 loss: 1.503239631652832 \n",
      "Epoch:  1\n",
      "1/36.0 loss: 1.2603518962860107 \n",
      "Epoch:  1\n",
      "2/36.0 loss: 1.2707217931747437 \n",
      "Epoch:  1\n",
      "3/36.0 loss: 1.3145799040794373 \n",
      "Epoch:  1\n",
      "4/36.0 loss: 1.3825627088546752 \n",
      "Epoch:  1\n",
      "5/36.0 loss: 1.3989834785461426 \n",
      "Epoch:  1\n",
      "6/36.0 loss: 1.3172224930354528 \n",
      "Epoch:  1\n",
      "7/36.0 loss: 1.4326949417591095 \n",
      "Epoch:  1\n",
      "8/36.0 loss: 1.378023472097185 \n",
      "Epoch:  1\n",
      "9/36.0 loss: 1.4096169173717499 \n",
      "Epoch:  1\n",
      "10/36.0 loss: 1.3842078284783796 \n",
      "Epoch:  1\n",
      "11/36.0 loss: 1.3899749666452408 \n",
      "Epoch:  1\n",
      "12/36.0 loss: 1.468306683577024 \n",
      "Epoch:  1\n",
      "13/36.0 loss: 1.4340163256440843 \n",
      "Epoch:  1\n",
      "14/36.0 loss: 1.4428147037823995 \n",
      "Epoch:  1\n",
      "15/36.0 loss: 1.422057244926691 \n",
      "Epoch:  1\n",
      "16/36.0 loss: 1.4581853396752302 \n",
      "Epoch:  1\n",
      "17/36.0 loss: 1.439686957332823 \n",
      "Epoch:  1\n",
      "18/36.0 loss: 1.4475009786455255 \n",
      "Epoch:  1\n",
      "19/36.0 loss: 1.4394472807645797 \n",
      "Epoch:  1\n",
      "20/36.0 loss: 1.4438557880265372 \n",
      "Epoch:  1\n",
      "21/36.0 loss: 1.4377187571742318 \n",
      "Epoch:  1\n",
      "22/36.0 loss: 1.4256151370380237 \n",
      "Epoch:  1\n",
      "23/36.0 loss: 1.4320497537652652 \n",
      "Epoch:  1\n",
      "24/36.0 loss: 1.412955822944641 \n",
      "Epoch:  1\n",
      "25/36.0 loss: 1.4026120075812707 \n",
      "Epoch:  1\n",
      "26/36.0 loss: 1.4005007214016385 \n",
      "Epoch:  1\n",
      "27/36.0 loss: 1.3982575493199485 \n",
      "Epoch:  1\n",
      "28/36.0 loss: 1.3939618365517978 \n",
      "Epoch:  1\n",
      "29/36.0 loss: 1.3823266903559366 \n",
      "Epoch:  1\n",
      "30/36.0 loss: 1.3711946241317257 \n",
      "Epoch:  1\n",
      "31/36.0 loss: 1.3642523139715195 \n",
      "Epoch:  1\n",
      "32/36.0 loss: 1.3603519454146877 \n",
      "Epoch:  1\n",
      "33/36.0 loss: 1.3564781862146713 \n",
      "Epoch:  1\n",
      "34/36.0 loss: 1.3538232905524117 \n",
      "Epoch:  1\n",
      "35/36.0 loss: 1.3449014359050326 \n",
      "Epoch:  2\n",
      "0/36.0 loss: 1.0922349691390991 \n",
      "Epoch:  2\n",
      "1/36.0 loss: 1.1060109734535217 \n",
      "Epoch:  2\n",
      "2/36.0 loss: 1.0898428360621135 \n",
      "Epoch:  2\n",
      "3/36.0 loss: 1.087084323167801 \n",
      "Epoch:  2\n",
      "4/36.0 loss: 1.081445050239563 \n",
      "Epoch:  2\n",
      "5/36.0 loss: 1.0880610744158428 \n",
      "Epoch:  2\n",
      "6/36.0 loss: 1.0857559783118111 \n",
      "Epoch:  2\n",
      "7/36.0 loss: 1.0955880284309387 \n",
      "Epoch:  2\n",
      "8/36.0 loss: 1.090025822321574 \n",
      "Epoch:  2\n",
      "9/36.0 loss: 1.087972104549408 \n",
      "Epoch:  2\n",
      "10/36.0 loss: 1.0887110450051047 \n",
      "Epoch:  2\n",
      "11/36.0 loss: 1.0935594538847606 \n",
      "Epoch:  2\n",
      "12/36.0 loss: 1.0978114696649404 \n",
      "Epoch:  2\n",
      "13/36.0 loss: 1.0971104672976904 \n",
      "Epoch:  2\n",
      "14/36.0 loss: 1.09575297832489 \n",
      "Epoch:  2\n",
      "15/36.0 loss: 1.099430412054062 \n",
      "Epoch:  2\n",
      "16/36.0 loss: 1.1039095415788538 \n",
      "Epoch:  2\n",
      "17/36.0 loss: 1.108211510711246 \n",
      "Epoch:  2\n",
      "18/36.0 loss: 1.1061801847658659 \n",
      "Epoch:  2\n",
      "19/36.0 loss: 1.1058172523975371 \n",
      "Epoch:  2\n",
      "20/36.0 loss: 1.1071723188672746 \n",
      "Epoch:  2\n",
      "21/36.0 loss: 1.1048720533197576 \n",
      "Epoch:  2\n",
      "22/36.0 loss: 1.1075003872747007 \n",
      "Epoch:  2\n",
      "23/36.0 loss: 1.110124294956525 \n",
      "Epoch:  2\n",
      "24/36.0 loss: 1.1084093523025513 \n",
      "Epoch:  2\n",
      "25/36.0 loss: 1.1045631766319275 \n",
      "Epoch:  2\n",
      "26/36.0 loss: 1.1010904047224257 \n",
      "Epoch:  2\n",
      "27/36.0 loss: 1.1013603465897697 \n",
      "Epoch:  2\n",
      "28/36.0 loss: 1.10040951186213 \n",
      "Epoch:  2\n",
      "29/36.0 loss: 1.0983570138613383 \n",
      "Epoch:  2\n",
      "30/36.0 loss: 1.0987853388632498 \n",
      "Epoch:  2\n",
      "31/36.0 loss: 1.0998171418905258 \n",
      "Epoch:  2\n",
      "32/36.0 loss: 1.0993135914658054 \n",
      "Epoch:  2\n",
      "33/36.0 loss: 1.099109081661 \n",
      "Epoch:  2\n",
      "34/36.0 loss: 1.0992245197296142 \n",
      "Epoch:  2\n",
      "35/36.0 loss: 1.097005569272571 \n",
      "Epoch:  3\n",
      "0/36.0 loss: 1.1847460269927979 \n",
      "Epoch:  3\n",
      "1/36.0 loss: 1.212454080581665 \n",
      "Epoch:  3\n",
      "2/36.0 loss: 1.183409055074056 \n",
      "Epoch:  3\n",
      "3/36.0 loss: 1.139529675245285 \n",
      "Epoch:  3\n",
      "4/36.0 loss: 1.12018141746521 \n",
      "Epoch:  3\n",
      "5/36.0 loss: 1.1090391675631206 \n",
      "Epoch:  3\n",
      "6/36.0 loss: 1.1122832979474748 \n",
      "Epoch:  3\n",
      "7/36.0 loss: 1.105947494506836 \n",
      "Epoch:  3\n",
      "8/36.0 loss: 1.0979287359449599 \n",
      "Epoch:  3\n",
      "9/36.0 loss: 1.0884934902191161 \n",
      "Epoch:  3\n",
      "10/36.0 loss: 1.084044640714472 \n",
      "Epoch:  3\n",
      "11/36.0 loss: 1.083616703748703 \n",
      "Epoch:  3\n",
      "12/36.0 loss: 1.08558988571167 \n",
      "Epoch:  3\n",
      "13/36.0 loss: 1.0836200032915388 \n",
      "Epoch:  3\n",
      "14/36.0 loss: 1.0804557164510091 \n",
      "Epoch:  3\n",
      "15/36.0 loss: 1.0867544263601303 \n",
      "Epoch:  3\n",
      "16/36.0 loss: 1.0900383696836584 \n",
      "Epoch:  3\n",
      "17/36.0 loss: 1.0960584746466742 \n",
      "Epoch:  3\n",
      "18/36.0 loss: 1.0923947786030017 \n",
      "Epoch:  3\n",
      "19/36.0 loss: 1.0904682755470276 \n",
      "Epoch:  3\n",
      "20/36.0 loss: 1.0909651347569056 \n",
      "Epoch:  3\n",
      "21/36.0 loss: 1.0890827829187566 \n",
      "Epoch:  3\n",
      "22/36.0 loss: 1.0907115988109424 \n",
      "Epoch:  3\n",
      "23/36.0 loss: 1.0929245700438817 \n",
      "Epoch:  3\n",
      "24/36.0 loss: 1.0909101247787476 \n",
      "Epoch:  3\n",
      "25/36.0 loss: 1.0860112699178548 \n",
      "Epoch:  3\n",
      "26/36.0 loss: 1.0822929739952087 \n",
      "Epoch:  3\n",
      "27/36.0 loss: 1.0821828139679772 \n",
      "Epoch:  3\n",
      "28/36.0 loss: 1.0804880549167764 \n",
      "Epoch:  3\n",
      "29/36.0 loss: 1.0780620396137237 \n",
      "Epoch:  3\n",
      "30/36.0 loss: 1.0783257734390996 \n",
      "Epoch:  3\n",
      "31/36.0 loss: 1.0790171679109335 \n",
      "Epoch:  3\n",
      "32/36.0 loss: 1.0787534334442832 \n",
      "Epoch:  3\n",
      "33/36.0 loss: 1.0780552222448236 \n",
      "Epoch:  3\n",
      "34/36.0 loss: 1.0781548108373369 \n",
      "Epoch:  3\n",
      "35/36.0 loss: 1.0762889534235 \n"
     ]
    }
   ],
   "source": [
    "train_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
