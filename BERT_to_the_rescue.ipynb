{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "7_FXneEfpdMM",
    "outputId": "ff8057f1-23dc-49b8-b11c-f68823bf6f90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_pretrained_bert in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: boto3 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.9.134)\n",
      "Requirement already satisfied: requests in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2.18.4)\n",
      "Requirement already satisfied: torch>=0.4.1 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pytorch_pretrained_bert) (4.32.1)\n",
      "Requirement already satisfied: regex in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2019.8.19)\n",
      "Requirement already satisfied: numpy in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.14.3)\n",
      "Requirement already satisfied: pandas in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pytorch-nlp) (0.23.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.134 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (1.12.134)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2018.4.16)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pandas->pytorch-nlp) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from pandas->pytorch-nlp) (2018.4)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch_pretrained_bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas->pytorch-nlp) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EnVIV6Vt8f4d",
    "outputId": "3c8e3d88-13b0-4652-973d-3fa2c023e48e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjaskolkambp/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from torch import nn\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUYrv06z8gaF"
   },
   "outputs": [],
   "source": [
    "rn.seed(321)\n",
    "np.random.seed(321)\n",
    "torch.manual_seed(321)\n",
    "torch.cuda.manual_seed(321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgkbhHcB17GY"
   },
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ur8i7boP6qtb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 84.1MB [00:10, 7.80MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = imdb_dataset(train=True, test=True)\n",
    "rn.shuffle(train_data)\n",
    "rn.shuffle(test_data)\n",
    "train_data = train_data[:1000]\n",
    "test_data = test_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"This movie brings back many memories of the classic cinema of old, where actors didn't have to take their clothes off to make viewers watch their film.<br /><br />Firstly I think the main plus point of this movie is the amazing chemistry between Shahid and Amrita, it is definitely the making of the film.<br /><br />I have seen lots of comments regarding the film being sickly sweet and overly slushy. In response to this, I think to a certain degree this is a correct analysis, however considering this is a Barjatya film I think that compared to MPK, HAHK, HSSH and MPKDH, it has been toned down significantly. HSSH was almost unbearable to watch in some places.<br /><br />In this film however, when the sentimental moments come along, you find yourself smiling, wishing the budding couple all the best and hoping that nothing bad happens to them.<br /><br />Another major plus point is the performances of Shahid and Amrita. Both have acted very well, especially Shahid who looks great in the film. Amrita looks simply stunning and should be taken seriously as a future major star.<br /><br />Although I really enjoyed the film as a whole, I do feel that it was too long. Some of the middle could have been trimmed off and it would maybe made even more of an impact. I also think the music, although it fits into the film when you see the situations is slightly old fashioned and the movie could have benefited if a more up-to-date soundtrack had been available. Although the picturisation of the songs Mujhe Haq Hain and Hamari Shaadi Mein are wonderful.<br /><br />All in all, I definitely recommend this film, its romantic, looks stunning and has a dramatic climax (I won't go into details, just in case you haven't seen it.<br /><br />PS. If you're prone to crying-take a tissue! (I needed several)\",\n",
       " 'sentiment': 'pos'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l1VENo6tqG7J",
    "outputId": "5f10799f-d17d-42fd-a4c2-c760b61f7ed8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 100, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n",
    "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
    "\n",
    "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, str)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_texts), type(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This movie brings back many memories of the classic cinema of old, where actors didn't have to take their clothes off to make viewers watch their film.<br /><br />Firstly I think the main plus point of this movie is the amazing chemistry between Shahid and Amrita, it is definitely the making of the film.<br /><br />I have seen lots of comments regarding the film being sickly sweet and overly slushy. In response to this, I think to a certain degree this is a correct analysis, however considering this is a Barjatya film I think that compared to MPK, HAHK, HSSH and MPKDH, it has been toned down significantly. HSSH was almost unbearable to watch in some places.<br /><br />In this film however, when the sentimental moments come along, you find yourself smiling, wishing the budding couple all the best and hoping that nothing bad happens to them.<br /><br />Another major plus point is the performances of Shahid and Amrita. Both have acted very well, especially Shahid who looks great in the film. Amrita looks simply stunning and should be taken seriously as a future major star.<br /><br />Although I really enjoyed the film as a whole, I do feel that it was too long. Some of the middle could have been trimmed off and it would maybe made even more of an impact. I also think the music, although it fits into the film when you see the situations is slightly old fashioned and the movie could have benefited if a more up-to-date soundtrack had been available. Although the picturisation of the songs Mujhe Haq Hain and Hamari Shaadi Mein are wonderful.<br /><br />All in all, I definitely recommend this film, its romantic, looks stunning and has a dramatic climax (I won't go into details, just in case you haven't seen it.<br /><br />PS. If you're prone to crying-take a tissue! (I needed several)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A sober, reflexive piece, a little miniature which blossoms into a magnificent humane pictorial sequence which goes beyond a mere dramatization for the screen. This quiet little story will hold you enthralled - if you do not have too many problems with the various Spanish accents ranging from Mexican to Peruvian, and Marisa Paredes' more authentic Iberian Peninsular usage! Garcíadiego has accomplished a perfect adaptation from the novel: even the grand maestro García Márquez should be proud of her superb work. And hats off to Arturo Ripstein who has so ably concerted the whole effort into a gem, a ruby, and so refined, so elegant, so sensitive, so touchingly.....<br /><br />El Coronel - Fernando Luján - is waiting to get his pension, while he continues to live in his ramshackle timber dwelling deep in the Colombian jungle (however, filmed elsewhere, NOT in Colombia) with his fighting cock and his wife (in that order?). And that is all there is to it.<br /><br />But, oh, so much more.... This film is a rhapsody.<br /><br />I must see this poetic little piece again as soon as possible. Worth the high side of 8 out of 10, which is very high on my scale.<br /><br />This is not light commercial Hollywood stuff.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ty24UrRjqIsb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 1811388.87B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "26trq3gIrJeG",
    "outputId": "196b2aa3-0176-4010-baf8-e650ca0da658"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'my', 'name', 'is', 'dim', '##a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('Hi my name is Dima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1k9rcOzQr5Zm",
    "outputId": "b9b59a4e-57bd-4e9c-fcd1-2a4ba1fadc0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
    "\n",
    "len(train_tokens), len(test_tokens)                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_tokens), type(train_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'brings',\n",
       " 'back',\n",
       " 'many',\n",
       " 'memories',\n",
       " 'of',\n",
       " 'the',\n",
       " 'classic',\n",
       " 'cinema',\n",
       " 'of',\n",
       " 'old',\n",
       " ',',\n",
       " 'where',\n",
       " 'actors',\n",
       " 'didn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'have',\n",
       " 'to',\n",
       " 'take',\n",
       " 'their',\n",
       " 'clothes',\n",
       " 'off',\n",
       " 'to',\n",
       " 'make',\n",
       " 'viewers',\n",
       " 'watch',\n",
       " 'their',\n",
       " 'film',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'firstly',\n",
       " 'i',\n",
       " 'think',\n",
       " 'the',\n",
       " 'main',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'of',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'the',\n",
       " 'amazing',\n",
       " 'chemistry',\n",
       " 'between',\n",
       " 'shah',\n",
       " '##id',\n",
       " 'and',\n",
       " 'am',\n",
       " '##rita',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'definitely',\n",
       " 'the',\n",
       " 'making',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'i',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'comments',\n",
       " 'regarding',\n",
       " 'the',\n",
       " 'film',\n",
       " 'being',\n",
       " 'sick',\n",
       " '##ly',\n",
       " 'sweet',\n",
       " 'and',\n",
       " 'overly',\n",
       " 'sl',\n",
       " '##ush',\n",
       " '##y',\n",
       " '.',\n",
       " 'in',\n",
       " 'response',\n",
       " 'to',\n",
       " 'this',\n",
       " ',',\n",
       " 'i',\n",
       " 'think',\n",
       " 'to',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'degree',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'correct',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'however',\n",
       " 'considering',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'bar',\n",
       " '##ja',\n",
       " '##tya',\n",
       " 'film',\n",
       " 'i',\n",
       " 'think',\n",
       " 'that',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'mp',\n",
       " '##k',\n",
       " ',',\n",
       " 'ha',\n",
       " '##h',\n",
       " '##k',\n",
       " ',',\n",
       " 'hs',\n",
       " '##sh',\n",
       " 'and',\n",
       " 'mp',\n",
       " '##k',\n",
       " '##dh',\n",
       " ',',\n",
       " 'it',\n",
       " 'has',\n",
       " 'been',\n",
       " 'toned',\n",
       " 'down',\n",
       " 'significantly',\n",
       " '.',\n",
       " 'hs',\n",
       " '##sh',\n",
       " 'was',\n",
       " 'almost',\n",
       " 'unbearable',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'in',\n",
       " 'some',\n",
       " 'places',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'in',\n",
       " 'this',\n",
       " 'film',\n",
       " 'however',\n",
       " ',',\n",
       " 'when',\n",
       " 'the',\n",
       " 'sentimental',\n",
       " 'moments',\n",
       " 'come',\n",
       " 'along',\n",
       " ',',\n",
       " 'you',\n",
       " 'find',\n",
       " 'yourself',\n",
       " 'smiling',\n",
       " ',',\n",
       " 'wishing',\n",
       " 'the',\n",
       " 'bud',\n",
       " '##ding',\n",
       " 'couple',\n",
       " 'all',\n",
       " 'the',\n",
       " 'best',\n",
       " 'and',\n",
       " 'hoping',\n",
       " 'that',\n",
       " 'nothing',\n",
       " 'bad',\n",
       " 'happens',\n",
       " 'to',\n",
       " 'them',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'another',\n",
       " 'major',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'is',\n",
       " 'the',\n",
       " 'performances',\n",
       " 'of',\n",
       " 'shah',\n",
       " '##id',\n",
       " 'and',\n",
       " 'am',\n",
       " '##rita',\n",
       " '.',\n",
       " 'both',\n",
       " 'have',\n",
       " 'acted',\n",
       " 'very',\n",
       " 'well',\n",
       " ',',\n",
       " 'especially',\n",
       " 'shah',\n",
       " '##id',\n",
       " 'who',\n",
       " 'looks',\n",
       " 'great',\n",
       " 'in',\n",
       " 'the',\n",
       " 'film',\n",
       " '.',\n",
       " 'am',\n",
       " '##rita',\n",
       " 'looks',\n",
       " 'simply',\n",
       " 'stunning',\n",
       " 'and',\n",
       " 'should',\n",
       " 'be',\n",
       " 'taken',\n",
       " 'seriously',\n",
       " 'as',\n",
       " 'a',\n",
       " 'future',\n",
       " 'major',\n",
       " 'star',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'although',\n",
       " 'i',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'the',\n",
       " 'film',\n",
       " 'as',\n",
       " 'a',\n",
       " 'whole',\n",
       " ',',\n",
       " 'i',\n",
       " 'do',\n",
       " 'feel',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'too',\n",
       " 'long',\n",
       " '.',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'could',\n",
       " 'have',\n",
       " 'been',\n",
       " 'trimmed',\n",
       " 'off',\n",
       " 'and',\n",
       " 'it',\n",
       " 'would',\n",
       " 'maybe',\n",
       " 'made',\n",
       " 'even',\n",
       " 'more',\n",
       " 'of',\n",
       " 'an',\n",
       " 'impact',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'think',\n",
       " 'the',\n",
       " 'music',\n",
       " ',',\n",
       " 'although',\n",
       " 'it',\n",
       " 'fits',\n",
       " 'into',\n",
       " 'the',\n",
       " 'film',\n",
       " 'when',\n",
       " 'you',\n",
       " 'see',\n",
       " 'the',\n",
       " 'situations',\n",
       " 'is',\n",
       " 'slightly',\n",
       " 'old',\n",
       " 'fashioned',\n",
       " 'and',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'could',\n",
       " 'have',\n",
       " 'benefited',\n",
       " 'if',\n",
       " 'a',\n",
       " 'more',\n",
       " 'up',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'date',\n",
       " 'soundtrack',\n",
       " 'had',\n",
       " 'been',\n",
       " 'available',\n",
       " '.',\n",
       " 'although',\n",
       " 'the',\n",
       " 'pic',\n",
       " '##tur',\n",
       " '##isation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'songs',\n",
       " 'mu',\n",
       " '##j',\n",
       " '##he',\n",
       " 'ha',\n",
       " '##q',\n",
       " 'hai',\n",
       " '##n',\n",
       " 'and',\n",
       " 'ham',\n",
       " '##ari',\n",
       " 'sha',\n",
       " '##adi',\n",
       " 'mein',\n",
       " 'are',\n",
       " 'wonderful',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'all',\n",
       " 'in',\n",
       " 'all',\n",
       " ',',\n",
       " 'i',\n",
       " 'definitely',\n",
       " 'recommend',\n",
       " 'this',\n",
       " 'film',\n",
       " ',',\n",
       " 'its',\n",
       " 'romantic',\n",
       " ',',\n",
       " 'looks',\n",
       " 'stunning',\n",
       " 'and',\n",
       " 'has',\n",
       " 'a',\n",
       " 'dramatic',\n",
       " 'climax',\n",
       " '(',\n",
       " 'i',\n",
       " 'won',\n",
       " \"'\",\n",
       " 't',\n",
       " 'go',\n",
       " 'into',\n",
       " 'details',\n",
       " ',',\n",
       " 'just',\n",
       " 'in',\n",
       " 'case',\n",
       " 'you',\n",
       " 'haven',\n",
       " \"'\",\n",
       " 't',\n",
       " 'seen',\n",
       " 'it',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'ps',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'prone',\n",
       " 'to',\n",
       " 'crying',\n",
       " '-',\n",
       " 'take',\n",
       " 'a',\n",
       " 'tissue',\n",
       " '!',\n",
       " '(',\n",
       " 'i',\n",
       " 'needed',\n",
       " 'several',\n",
       " ')',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9Ca7KKnhuT5c",
    "outputId": "69ba3b87-4d86-448b-c9cb-7be8361182f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 512), (100, 512))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "train_tokens_ids.shape, test_tokens_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  101,  2023,  3185,  7545,  2067,  2116,  5758,  1997,  1996,\n",
       "         4438,  5988,  1997,  2214,  1010,  2073,  5889,  2134,  1005,\n",
       "         1056,  2031,  2000,  2202,  2037,  4253,  2125,  2000,  2191,\n",
       "         7193,  3422,  2037,  2143,  1012,  1026,  7987,  1013,  1028,\n",
       "         1026,  7987,  1013,  1028, 15847,  1045,  2228,  1996,  2364,\n",
       "         4606,  2391,  1997,  2023,  3185,  2003,  1996,  6429,  6370,\n",
       "         2090,  7890,  3593,  1998,  2572, 17728,  1010,  2009,  2003,\n",
       "         5791,  1996,  2437,  1997,  1996,  2143,  1012,  1026,  7987,\n",
       "         1013,  1028,  1026,  7987,  1013,  1028,  1045,  2031,  2464,\n",
       "         7167,  1997,  7928,  4953,  1996,  2143,  2108,  5305,  2135,\n",
       "         4086,  1998, 15241, 22889, 20668,  2100,  1012,  1999,  3433,\n",
       "         2000,  2023,  1010,  1045,  2228,  2000,  1037,  3056,  3014,\n",
       "         2023,  2003,  1037,  6149,  4106,  1010,  2174,  6195,  2023,\n",
       "         2003,  1037,  3347,  3900, 21426,  2143,  1045,  2228,  2008,\n",
       "         4102,  2000,  6131,  2243,  1010,  5292,  2232,  2243,  1010,\n",
       "        26236,  4095,  1998,  6131,  2243, 16425,  1010,  2009,  2038,\n",
       "         2042, 27604,  2091,  6022,  1012, 26236,  4095,  2001,  2471,\n",
       "        24257,  2000,  3422,  1999,  2070,  3182,  1012,  1026,  7987,\n",
       "         1013,  1028,  1026,  7987,  1013,  1028,  1999,  2023,  2143,\n",
       "         2174,  1010,  2043,  1996, 23069,  5312,  2272,  2247,  1010,\n",
       "         2017,  2424,  4426,  5629,  1010, 10261,  1996, 13007,  4667,\n",
       "         3232,  2035,  1996,  2190,  1998,  5327,  2008,  2498,  2919,\n",
       "         6433,  2000,  2068,  1012,  1026,  7987,  1013,  1028,  1026,\n",
       "         7987,  1013,  1028,  2178,  2350,  4606,  2391,  2003,  1996,\n",
       "         4616,  1997,  7890,  3593,  1998,  2572, 17728,  1012,  2119,\n",
       "         2031,  6051,  2200,  2092,  1010,  2926,  7890,  3593,  2040,\n",
       "         3504,  2307,  1999,  1996,  2143,  1012,  2572, 17728,  3504,\n",
       "         3432, 14726,  1998,  2323,  2022,  2579,  5667,  2004,  1037,\n",
       "         2925,  2350,  2732,  1012,  1026,  7987,  1013,  1028,  1026,\n",
       "         7987,  1013,  1028,  2348,  1045,  2428,  5632,  1996,  2143,\n",
       "         2004,  1037,  2878,  1010,  1045,  2079,  2514,  2008,  2009,\n",
       "         2001,  2205,  2146,  1012,  2070,  1997,  1996,  2690,  2071,\n",
       "         2031,  2042, 21920,  2125,  1998,  2009,  2052,  2672,  2081,\n",
       "         2130,  2062,  1997,  2019,  4254,  1012,  1045,  2036,  2228,\n",
       "         1996,  2189,  1010,  2348,  2009, 16142,  2046,  1996,  2143,\n",
       "         2043,  2017,  2156,  1996,  8146,  2003,  3621,  2214, 13405,\n",
       "         1998,  1996,  3185,  2071,  2031, 19727,  2065,  1037,  2062,\n",
       "         2039,  1011,  2000,  1011,  3058,  6050,  2018,  2042,  2800,\n",
       "         1012,  2348,  1996, 27263, 20689,  6648,  1997,  1996,  2774,\n",
       "        14163,  3501,  5369,  5292,  4160, 15030,  2078,  1998, 10654,\n",
       "         8486, 21146, 17190, 24182,  2024,  6919,  1012,  1026,  7987,\n",
       "         1013,  1028,  1026,  7987,  1013,  1028,  2035,  1999,  2035,\n",
       "         1010,  1045,  5791, 16755,  2023,  2143,  1010,  2049,  6298,\n",
       "         1010,  3504, 14726,  1998,  2038,  1037,  6918, 14463,  1006,\n",
       "         1045,  2180,  1005,  1056,  2175,  2046,  4751,  1010,  2074,\n",
       "         1999,  2553,  2017,  4033,  1005,  1056,  2464,  2009,  1012,\n",
       "         1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  8827,\n",
       "         1012,  2065,  2017,  1005,  2128, 13047,  2000,  6933,  1011,\n",
       "         2202,  1037,  8153,   999,  1006,  1045,  2734,  2195,  1007,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0]), (512,))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_ids[0], train_tokens_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F7POtHuIOV-6",
    "outputId": "a5447340-6da3-4f2c-d284-098c4c906c47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000,), (100,), 0.489, 0.5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = np.array(train_labels) == 'pos'\n",
    "test_y = np.array(test_labels) == 'pos'\n",
    "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-xXMEqXOWTE"
   },
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_masks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2K4JQMFo1-_S"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdzjl_WlwpKr"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "9jyb-hJ0xAgG",
    "outputId": "4cc5ca8c-646f-410f-d5c5-998fdf82f00e"
   },
   "outputs": [],
   "source": [
    "baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression()).fit(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9IzjAX_2VLf"
   },
   "outputs": [],
   "source": [
    "baseline_predicted = baseline_model.predict(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "QnsCRIaQ3GPQ",
    "outputId": "46417074-2870-4a52-9a6a-307bde86b704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.81      0.84      0.82        50\n",
      "        pos       0.83      0.80      0.82        50\n",
      "\n",
      "avg / total       0.82      0.82      0.82       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, baseline_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_hEhebQ3YqI"
   },
   "source": [
    "# Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E234ByBa3Qtb"
   },
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(768,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 2028, 1011,  ...,    0,    0,    0],\n",
       "         [ 101, 2028, 1997,  ...,    0,    0,    0],\n",
       "         [ 101, 2893, 2439,  ...,    0,    0,    0],\n",
       "         [ 101, 2887, 3419,  ..., 2005, 1996,  102]]),\n",
       " tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]])]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]), torch.Size([4, 512]), torch.Size([4, 1]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape, x[1].shape, x[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2028,  1011,  3602,  4038,  2008,  2763,  4520,  2715,  2154,\n",
       "        10469,  2015,  1005,  4091,  2006,  3341,  1012,  2533,  3573,  7805,\n",
       "        22396,  7867,  2003,  1999,  2293,  2007,  1996,  2801,  1997, 10834,\n",
       "         1998,  3510,  1010, 22866,  2014,  8069,  2006,  2308,  1005,  1055,\n",
       "         7298,  2127,  2016, 16794,  3565,  1011, 17395,  1999,  1996,  2433,\n",
       "         1997, 21185,  5065, 20533,  3946,  1012,  1996,  2717,  1997,  1996,\n",
       "         2143, 20228, 20620,  2013,  2028, 20228,  6977,  2000,  1996,  2279,\n",
       "         2004,  1996, 21660,  7867,  7323,  2015,  2014, 12907,  1012,  1045,\n",
       "         3984,  1996,  2773,  1000,  2566,  4801,  1000,  2074,  2055,  7680,\n",
       "         1005,  1055,  2039,  7867,  1005,  1055,  3921,  2000,  1996,  2535,\n",
       "         1012,  2016,  2515,  2031,  1037, 11951,  2868,  1010,  2021,  2044,\n",
       "         2322,  2781,  1997, 24443, 21885,  2075,  2014, 21418, 14900,  1010,\n",
       "         1045,  2211,  2000, 26641,  1012,  3946,  1005,  1055,  2535,  2003,\n",
       "        10468,  3905,  1998, 15718,  2010,  5156, 22012,  1012,  2045,  2003,\n",
       "         2028,  3496,  1010,  2174,  1010,  2008,  2471, 18340,  2015,  2023,\n",
       "        10944,  6912,  1012,  7867, 10861,  5134,  1996,  5292, 21112,  2015,\n",
       "         3946,  2206,  2010,  8835,  2000,  1037,  2282,  3993,  1997, 19416,\n",
       "         6456,  1012,  2182,  2014,  2566,  4801,  5450,  2038,  2019,  4895,\n",
       "        14821,  2094,  4840,  2791,  2008,  2003,  2428,  3243,  9487,  1010,\n",
       "         1998,  2018,  1996,  2537,  2025,  7503,  2256, 27518,  1999,  2008,\n",
       "        27999,  5861,  2005,  3938,  2781,  1010,  1996,  2143,  2453,  2031,\n",
       "        18779,  2000,  2062,  2084,  1037,  3057,  1005,  3409,  2154,  1011,\n",
       "         3959,  1010, 12800,  3882,  1012,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, pooled = model(x[0].long(), x[1].long(), output_all_encoded_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.2035e-01, -1.2935e-01, -6.3447e-01, -3.8974e-01,  4.2116e-01,\n",
       "        -5.5024e-02, -9.3580e-01,  1.4767e-01, -5.9223e-01, -4.6779e-01,\n",
       "         2.5108e-01,  3.9842e-01,  3.1596e-01,  5.0302e-01, -3.0484e-01,\n",
       "         7.5943e-01,  8.6045e-01, -3.5990e-02,  6.9738e-03,  9.6940e-01,\n",
       "         3.3081e-01,  9.9329e-01,  1.9115e-02,  3.4884e-01,  2.6564e-01,\n",
       "         4.4706e-01,  2.5722e-02, -2.3618e-02, -1.0260e-01,  4.6844e-01,\n",
       "         6.2713e-01, -3.7711e-02, -6.6075e-01, -2.5164e-01, -8.9383e-01,\n",
       "        -2.2646e-01, -1.0696e-02,  4.0202e-01, -7.4280e-02,  3.4344e-01,\n",
       "         2.4658e-01, -2.4652e-02,  8.9589e-01,  6.0270e-01, -3.4519e-02,\n",
       "         1.0284e-01,  9.0841e-01,  1.4352e-01,  4.3273e-01,  7.6508e-01,\n",
       "         4.6201e-01,  8.9849e-01, -1.2410e-01, -1.2699e-01, -5.5505e-02,\n",
       "        -3.2303e-01, -2.6756e-01,  9.5763e-03, -4.8406e-02,  4.9331e-03,\n",
       "        -3.1580e-01,  2.6484e-01, -6.0025e-01,  5.4551e-01,  1.9510e-01,\n",
       "         7.8348e-01, -8.1185e-02,  4.8980e-02,  1.1712e-01, -3.0012e-01,\n",
       "        -7.0913e-01, -1.6695e-02, -3.6874e-01, -3.7691e-01,  4.3575e-01,\n",
       "         2.5691e-01, -7.7222e-01, -4.7898e-01,  6.9013e-01, -2.7781e-01,\n",
       "         8.5374e-01,  4.6511e-01,  6.5032e-01, -1.9981e-01,  5.7287e-01,\n",
       "         4.0138e-01,  1.7947e-01, -2.1081e-02, -4.8591e-01,  9.1677e-02,\n",
       "        -1.0841e-01,  1.4386e-01,  8.8290e-01,  6.9098e-01, -6.8937e-02,\n",
       "        -3.7767e-01,  1.0439e-01, -5.9416e-01,  6.1504e-03, -3.4988e-01,\n",
       "         1.6260e-01,  8.5160e-02, -8.4419e-02,  1.5541e-01,  2.6549e-01,\n",
       "        -2.9003e-01,  7.2740e-01,  7.4168e-01, -3.7889e-01, -1.0006e-02,\n",
       "         7.0612e-01, -2.8795e-01,  4.5810e-03,  3.1697e-01, -2.0520e-02,\n",
       "        -1.2486e-01, -4.4868e-01, -7.3318e-01, -4.6576e-01, -2.3340e-01,\n",
       "         8.2590e-02,  5.2626e-02, -6.8288e-02, -7.9399e-01, -1.6138e-01,\n",
       "        -2.1237e-01, -5.0619e-01,  3.3056e-01,  4.4818e-01,  1.1508e-01,\n",
       "        -6.5146e-01, -2.1252e-01, -3.1395e-01, -6.0318e-01, -1.7783e-02,\n",
       "        -2.3467e-01, -3.1359e-01,  5.5394e-01, -1.5779e-01,  2.8868e-02,\n",
       "         1.6529e-01, -5.5757e-01,  5.6233e-01, -6.4416e-02,  3.1813e-01,\n",
       "        -9.2240e-01,  7.9696e-01, -1.3102e-01,  1.2267e-01, -2.0086e-02,\n",
       "        -1.3919e-01,  3.2465e-01, -3.0034e-01,  3.9600e-01, -2.0451e-01,\n",
       "         1.5432e-01, -5.6140e-01, -5.7134e-01, -1.9432e-01,  1.2696e-01,\n",
       "         4.8436e-01, -3.8047e-01, -2.1632e-01, -1.0736e-01, -2.7246e-01,\n",
       "         2.5759e-01, -2.0827e-01, -1.6353e-01,  2.6559e-01, -1.0082e-01,\n",
       "        -1.1179e-01, -2.0294e-01, -7.9916e-01, -3.5946e-01, -1.4722e-01,\n",
       "        -2.4831e-01,  1.3251e-01, -2.3301e-01,  3.6346e-02,  5.3031e-02,\n",
       "        -8.0579e-02, -7.8573e-03, -5.7223e-01,  3.9095e-01,  2.4388e-01,\n",
       "         1.5956e-01, -7.6236e-01,  8.5970e-01,  5.2423e-01,  4.6321e-01,\n",
       "        -6.9982e-01, -1.5178e-02,  3.4510e-01,  2.0176e-01, -2.4569e-01,\n",
       "         2.5801e-01, -2.7950e-01, -5.7627e-01,  6.3436e-02, -8.2844e-02,\n",
       "         1.7966e-03,  1.5739e-01, -9.6781e-01, -8.8644e-01,  4.4230e-01,\n",
       "         5.4215e-02,  4.2653e-01, -3.6144e-01, -7.1112e-02, -3.4200e-01,\n",
       "         3.3576e-03,  1.0911e-01, -5.8881e-01,  4.0160e-01,  3.2150e-01,\n",
       "        -3.4774e-01, -7.4329e-02,  4.3021e-01, -5.6261e-02,  1.5085e-01,\n",
       "         3.6291e-01, -7.9772e-01, -4.0202e-01,  2.0450e-01,  9.7368e-01,\n",
       "         8.0919e-02, -2.0184e-01, -1.3451e-01,  2.2911e-01,  9.5634e-02,\n",
       "         9.6426e-01,  9.1793e-01, -1.2999e-01, -9.8523e-01,  8.4441e-01,\n",
       "        -9.1511e-01, -1.9592e-01, -7.1149e-01, -7.0125e-02, -7.6787e-01,\n",
       "        -1.9175e-01, -4.5655e-01,  5.6164e-01, -2.8095e-01,  3.5925e-01,\n",
       "        -6.8094e-01,  9.1636e-01,  1.3743e-01, -1.0562e-01,  1.1109e-01,\n",
       "         3.9849e-01,  6.6779e-01,  1.7082e-01,  1.5859e-01,  1.8150e-01,\n",
       "        -9.9766e-01, -9.8700e-01, -1.9247e-01, -9.4695e-01,  1.2609e-01,\n",
       "        -6.6505e-01,  4.7543e-01,  3.3107e-01,  6.8890e-01, -7.8585e-02,\n",
       "         6.5653e-01,  4.3498e-02, -8.5924e-01, -1.4839e-01, -8.3068e-01,\n",
       "        -5.7189e-01,  7.2069e-01, -4.7333e-01, -3.5319e-01, -1.0390e-01,\n",
       "         3.2068e-02, -3.8806e-01, -3.0213e-02,  4.0136e-01,  2.8278e-01,\n",
       "        -3.3676e-01,  4.8047e-02, -8.0452e-01, -9.4187e-01,  9.6799e-01,\n",
       "        -2.2397e-01, -3.9843e-01, -8.9615e-01, -9.8069e-01, -9.2516e-01,\n",
       "         7.4708e-01, -7.4819e-01, -1.5205e-01,  5.0170e-01,  1.5447e-01,\n",
       "        -5.7229e-01,  3.4476e-01, -1.9500e-01,  1.3820e-01,  4.5963e-01,\n",
       "         4.6175e-01,  8.0672e-02, -9.7835e-01, -3.0527e-01, -7.7015e-01,\n",
       "        -1.4404e-01, -7.1580e-01,  2.1633e-01, -3.6002e-01,  1.9769e-01,\n",
       "        -5.3905e-02,  3.8598e-01,  8.1379e-02,  3.3801e-01, -6.2321e-01,\n",
       "         7.1833e-01, -5.8831e-01, -7.2453e-01,  9.5500e-01,  1.2066e-01,\n",
       "         1.4061e-01,  4.5855e-01,  4.1469e-02,  1.5291e-01,  1.3554e-02,\n",
       "        -2.8181e-01, -1.8170e-01, -2.3215e-01, -3.0475e-01, -1.2288e-02,\n",
       "         6.2199e-02,  7.8305e-01, -1.4468e-01,  8.3917e-01, -7.7845e-01,\n",
       "         6.7971e-01, -2.5032e-01,  7.7681e-01, -3.8797e-01,  2.6478e-01,\n",
       "        -3.6480e-01, -7.4313e-01, -1.6382e-01, -2.0111e-01,  1.6513e-01,\n",
       "         2.8719e-01,  6.9098e-01,  7.3026e-01,  2.7008e-01, -2.1025e-01,\n",
       "        -4.5348e-01, -5.2642e-01,  4.5095e-01, -2.8209e-01, -7.3599e-01,\n",
       "         5.2678e-01, -8.7475e-02, -4.3741e-01,  4.1527e-01, -7.0910e-02,\n",
       "        -5.1381e-01,  3.4050e-01,  6.4974e-01, -6.7227e-01,  8.6530e-01,\n",
       "         6.1397e-01, -3.0815e-01, -1.2208e-01,  1.1813e-01,  6.5318e-01,\n",
       "         6.4205e-01, -7.0941e-03, -3.3121e-01,  6.0196e-02,  8.6512e-01,\n",
       "        -2.8109e-01, -3.1907e-01,  2.0660e-01, -7.0688e-02,  9.8081e-01,\n",
       "        -6.8867e-01, -1.6164e-01, -3.1942e-01, -3.1766e-02,  7.2635e-01,\n",
       "         1.1036e-01, -3.1533e-01, -8.4464e-02, -1.8466e-01, -6.3979e-01,\n",
       "         1.9126e-01,  3.3863e-01,  4.9833e-01,  3.5025e-03,  3.5747e-01,\n",
       "         4.7130e-01, -1.1905e-01,  2.0037e-01,  1.6365e-01,  2.7528e-02,\n",
       "        -5.8563e-02,  5.0109e-01,  6.1392e-01,  6.7846e-01, -6.9469e-01,\n",
       "        -5.8920e-01,  3.8419e-02,  1.4206e-01, -5.8891e-01, -9.5266e-02,\n",
       "         1.8636e-01,  4.9560e-01,  4.8882e-01, -7.9349e-02, -8.0104e-01,\n",
       "         4.1599e-01,  2.6014e-01, -4.5840e-03,  7.9317e-01, -9.2488e-01,\n",
       "        -3.9892e-01,  1.9488e-02, -2.6691e-01, -2.8366e-02, -4.4134e-01,\n",
       "         1.0547e-01, -4.3834e-02,  1.6440e-01,  7.6061e-01,  2.4671e-01,\n",
       "         2.2701e-01,  3.3487e-01,  2.0949e-01, -9.1888e-01, -6.2964e-01,\n",
       "         9.9206e-01, -8.0023e-01, -9.8554e-01,  5.7797e-01, -1.9133e-01,\n",
       "         1.3731e-01, -4.3195e-01,  1.9801e-01,  1.8745e-01,  3.0701e-01,\n",
       "         3.5297e-01,  8.0042e-01,  8.7610e-01,  2.7898e-02, -8.4996e-01,\n",
       "        -8.8938e-01, -1.3842e-01, -7.2644e-01,  2.6237e-01,  1.9881e-01,\n",
       "        -2.5502e-01, -1.3967e-01, -4.3294e-02, -1.1584e-01, -2.6434e-02,\n",
       "         1.7615e-01, -2.4854e-02,  6.9168e-01,  6.1475e-01,  4.6275e-01,\n",
       "        -5.9979e-01,  3.4787e-02, -6.6558e-02,  4.6702e-01, -8.3883e-01,\n",
       "         6.2548e-01, -7.0461e-01, -3.7224e-02, -3.6711e-01, -4.8383e-01,\n",
       "         5.4759e-01, -5.3505e-01, -7.9539e-01, -1.6353e-02,  3.0126e-01,\n",
       "         9.3983e-01,  3.3614e-01, -5.4999e-01, -2.9983e-01,  9.9545e-01,\n",
       "        -9.0544e-02,  6.0349e-02,  1.9175e-01,  7.0069e-01, -3.2549e-01,\n",
       "         1.3024e-01,  2.0764e-02,  5.4644e-02,  1.2231e-01, -9.9368e-01,\n",
       "         2.8375e-01, -1.5570e-01, -6.7413e-02, -2.2779e-01, -2.5254e-01,\n",
       "         9.8421e-01,  7.1456e-01,  1.4043e-01,  4.3738e-02, -4.4491e-01,\n",
       "        -3.1873e-01,  7.8135e-01,  9.8480e-01,  2.2114e-01,  8.9613e-01,\n",
       "         1.5635e-01,  6.2940e-02,  7.8230e-02, -9.4438e-01, -1.3448e-01,\n",
       "         4.2993e-02, -2.7921e-02,  2.9304e-04, -6.9930e-02, -3.6204e-01,\n",
       "        -2.9771e-01,  3.9803e-01,  4.7434e-02,  1.6375e-01,  9.8722e-01,\n",
       "        -1.9449e-01,  3.5361e-02,  3.6369e-01, -1.5142e-01, -9.9872e-01,\n",
       "        -8.8285e-02, -1.7555e-01, -3.0956e-01, -6.9364e-02, -1.0996e-01,\n",
       "        -1.1529e-01, -7.6328e-01, -3.6433e-01,  8.1054e-01, -1.3966e-01,\n",
       "        -3.2168e-01,  1.5556e-01,  2.0701e-03, -5.5619e-02,  3.5479e-01,\n",
       "        -6.4780e-01,  1.2728e-01,  2.0945e-01, -3.3690e-01,  2.3558e-01,\n",
       "         1.5782e-01,  3.6074e-01, -5.2773e-01,  3.6555e-02, -8.9279e-02,\n",
       "        -1.1687e-01,  6.7222e-02, -3.5469e-02, -5.6201e-01, -7.2889e-01,\n",
       "         1.1707e-01, -6.3323e-01, -2.0452e-01,  2.0833e-01, -8.1044e-02,\n",
       "        -2.3041e-01, -2.9773e-01,  9.6396e-02, -1.1558e-01,  4.9071e-02,\n",
       "        -2.8289e-01,  3.4438e-03, -2.3459e-01, -9.9169e-01,  8.9894e-02,\n",
       "         2.5651e-01,  6.4794e-01,  3.7782e-01, -1.9557e-01,  2.5866e-01,\n",
       "         6.1735e-01,  3.5675e-01,  6.2744e-01, -5.7862e-01,  1.5221e-01,\n",
       "        -9.0867e-02,  8.3847e-02,  3.2466e-01,  1.1363e-01,  2.6451e-01,\n",
       "        -3.8984e-02,  2.8022e-02, -7.3941e-01,  4.9580e-02, -1.2593e-01,\n",
       "        -6.9152e-01, -1.3943e-01,  3.9013e-01,  7.6209e-01,  7.2816e-03,\n",
       "        -1.3625e-02,  9.0780e-01,  9.6538e-01, -8.9776e-02,  1.5946e-01,\n",
       "         1.9506e-01,  5.4082e-02,  3.0865e-01, -2.9529e-02,  1.3956e-01,\n",
       "        -2.7918e-01, -4.5134e-01, -9.6605e-01, -7.3960e-01,  1.8736e-01,\n",
       "         4.7846e-02,  1.0029e-02,  1.5578e-01,  7.6250e-01,  3.3950e-01,\n",
       "        -2.2546e-01, -8.0907e-01, -9.6777e-01, -2.3877e-01,  2.6941e-02,\n",
       "         2.6955e-01, -9.3244e-01, -9.9330e-02,  8.1321e-02,  1.0555e-01,\n",
       "         9.8141e-02, -3.4497e-01,  5.4282e-01,  8.0524e-02,  3.4597e-01,\n",
       "        -3.8962e-02,  4.2105e-02,  4.1892e-01, -9.3380e-02, -1.0456e-01,\n",
       "        -5.8244e-01, -8.3493e-02,  1.3772e-01, -4.5054e-01,  1.6673e-01,\n",
       "         3.8281e-01, -3.0781e-01, -3.3961e-01,  2.4395e-01, -1.5070e-01,\n",
       "         2.0271e-02,  4.1071e-01,  2.0747e-01, -2.5387e-02,  4.3405e-02,\n",
       "        -7.7823e-01,  3.2010e-01,  6.0682e-02,  7.3833e-01,  5.5513e-02,\n",
       "         2.7777e-01,  3.8006e-01,  9.4722e-02, -9.9130e-02,  9.4392e-01,\n",
       "         8.6712e-02,  9.4102e-02, -4.8218e-01,  5.3289e-02, -1.6958e-01,\n",
       "        -3.3345e-02, -1.9428e-01, -3.5937e-01, -7.9103e-01, -9.8102e-01,\n",
       "        -4.1891e-01,  2.9580e-01, -3.7774e-01,  9.6398e-01,  3.0140e-01,\n",
       "         2.1825e-01,  4.5462e-02, -3.0976e-01, -8.2118e-01, -9.8739e-01,\n",
       "        -7.0685e-01, -4.6895e-01, -4.0479e-01,  1.1374e-01, -5.5691e-01,\n",
       "        -7.4676e-01,  2.2083e-01, -6.6463e-01, -8.5352e-01, -6.0855e-01,\n",
       "        -1.6034e-01,  7.4179e-01, -4.6696e-01,  6.6253e-02,  4.1451e-01,\n",
       "        -2.0475e-01,  1.6805e-01, -3.8191e-01,  9.1342e-01,  9.5643e-02,\n",
       "        -2.7733e-01, -4.1555e-02, -1.7304e-01,  9.5853e-03,  4.5165e-02,\n",
       "        -2.6177e-01,  6.1439e-01, -2.1749e-01,  7.3100e-01, -6.2178e-01,\n",
       "        -3.6910e-01,  4.7251e-01, -1.4694e-01,  3.8577e-01,  1.4996e-02,\n",
       "        -5.2646e-01,  1.2918e-01, -9.7859e-01,  3.6897e-01, -4.5696e-01,\n",
       "         3.2694e-01, -3.5199e-01,  8.5055e-01, -2.2616e-01, -4.5256e-02,\n",
       "         2.3186e-02,  2.4610e-01, -7.2127e-01, -2.6081e-01,  2.1693e-02,\n",
       "         2.7627e-01, -4.3584e-01, -1.7216e-01,  9.8004e-01, -3.9168e-02,\n",
       "        -3.1925e-01, -1.2447e-02, -8.8563e-02,  3.0099e-01,  4.6182e-01,\n",
       "        -8.4697e-01, -6.8223e-03, -7.8507e-01,  6.6933e-01,  2.3604e-01,\n",
       "         6.1835e-01, -2.3492e-01,  8.6088e-01,  6.2585e-02,  4.4618e-01,\n",
       "         6.2980e-01, -6.3194e-02, -2.8437e-01,  6.5903e-01, -6.5407e-01,\n",
       "         3.3285e-01, -4.2310e-01,  5.2701e-01,  1.7224e-01,  7.5143e-01,\n",
       "        -3.6167e-01, -6.9024e-02, -1.1910e-01], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = linear(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2232], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = sig(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4444],\n",
       "        [0.2773],\n",
       "        [0.5422],\n",
       "        [0.6100]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ED9SE1Ka8W9x",
    "outputId": "ab8b549b-137b-491a-cf6f-f20718dd73ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FNZ_3auqDbjl",
    "outputId": "1c20ee58-abb4-4bf2-f79a-851ab54c31eb"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a cuda device, but got: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-15d720087643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000000\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'M'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mmemory_allocated\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mdetails\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0mmanagement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_memoryAllocated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/cuda/_utils.py\u001b[0m in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdev_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected a cuda device, but got: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mdevice_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a cuda device, but got: cpu"
     ]
    }
   ],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sf9n8zouENRi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:20<00:00, 19801777.14B/s]\n"
     ]
    }
   ],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "#bert_clf = bert_clf.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LbHkaJuZEkYr",
    "outputId": "dff4b8e7-db91-46de-8b8c-bf0cddccaba9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'439.065088M'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LOQ-870M7VWy",
    "outputId": "f7c010bf-eb70-413f-96db-c0eb14769cfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 512]), torch.Size([3, 512, 768]), torch.Size([3, 768]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(train_tokens_ids[:3]).to(device)\n",
    "y, pooled = bert_clf.bert(x, output_all_encoded_layers=False)\n",
    "x.shape, y.shape, pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "LCb_pK4X7hb9",
    "outputId": "77237549-1b51-4d51-a11e-84e53fa82cb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.431305 ],\n",
       "       [0.4216192],\n",
       "       [0.5204962]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bert_clf(x)\n",
    "y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MUm-gFCuFkoI",
    "outputId": "4cac9ca3-1080-4307-81a4-d2a7d7618eab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6698.214912M'"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KzzsUZOUFcxp",
    "outputId": "fdb38786-4df8-473d-cf45-f635741a868e"
   },
   "outputs": [],
   "source": [
    "y, x, pooled = None, None, None\n",
    "#torch.cuda.empty_cache()\n",
    "#str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9LPIYcn99r8"
   },
   "source": [
    "# Fine-tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUkXhM1k_TAl"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_y.reshape(-1,1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jGwV0yqg_o2u",
    "outputId": "236eae60-f405-4f2b-89be-1164fb924b91"
   },
   "outputs": [],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
    "\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "#str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Yl2JpCe9YAu"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JF_QD0naS8EQ"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
    "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b28PcoDh_cyd"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6yIChYvBP5F"
   },
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mqh8tCl4AFjo",
    "outputId": "15c746cf-ba17-483a-d0ff-0bcbf9e96431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\r",
      "23/250.0 loss: 0.6890591283639272 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b53939e8f10c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mbert_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "        #print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        \n",
    "        loss_func = nn.BCELoss()\n",
    "\n",
    "        batch_loss = loss_func(logits, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "        \n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        \n",
    "\n",
    "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GHFWhkRYHv5l"
   },
   "outputs": [],
   "source": [
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vg_sX9BjooL-",
    "outputId": "2e85f485-c125-4ed6-bb8a-07ed708c54f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bert_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "-DmIJqUnkVM8",
    "outputId": "de735d0b-ad04-4d81-ef14-c6394bab92d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.76      0.81        50\n",
      "        True       0.79      0.88      0.83        50\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.82      0.82      0.82       100\n",
      "weighted avg       0.82      0.82      0.82       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, bert_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIBvoExLpOne"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT to the rescue.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
